{
    "[ 0.07455722  0.00729453  0.05088441 -0.01883614  0.05503938 -0.00413508\n  0.047888   -0.05873418  0.00531023  0.0169203 ]": "### Moving stuff from local to ssh server",
    "[ 0.04560048  0.02803576 -0.0753657  -0.07124253 -0.03832657  0.04939977\n  0.01259317 -0.0035406   0.02053352 -0.00163674]": "If using ssh tunnel: ssh -J vbo084@login",
    "[-0.09977662 -0.00984676 -0.05027418  0.00059642 -0.10330332  0.00977545\n  0.09638169  0.04252345 -0.01964624 -0.07716803]": "uib",
    "[-0.0185862   0.01851808 -0.06167074 -0.07029226 -0.01024596  0.03094052\n  0.0678331   0.06184657  0.0463132  -0.02741058]": "no vbo084@birget",
    "[-0.08237414  0.00767756  0.05781966  0.01725497  0.01703362  0.01029584\n  0.07164257 -0.00083997 -0.1187909  -0.09001877]": "no:",
    "[ 0.03254033  0.00785288 -0.03444591 -0.01530496  0.03261448  0.05425656\n  0.03292803  0.03734431 -0.06238731 -0.01098897]": "scp /path/to/local/file username@ssh_server:/path/on/server",
    "[ 0.00059327 -0.03702482  0.01180457 -0.01304158  0.04135282 -0.03069008\n -0.02704161 -0.0410183  -0.08489213 -0.04682589]": "# 10/07/24\n\n### Things Todo:\n- [ ] Try to benchmark other models in the Family Ontology dataset:\n\n### Thoughts of the day:",
    "[-0.09922481  0.01457155 -0.01956407 -0.00389503  0.06538109 -0.0567368\n  0.00300889 -0.06007228 -0.03375421  0.01231908]": "My GitHub page needs some polishing",
    "[-0.0983175   0.06788052  0.0588074   0.00729881  0.09603973  0.04304165\n  0.0951979   0.0170894  -0.05313643 -0.06680949]": "# 11/07/24\n### Things Todo:\n\n\n### Thoughts of the day:",
    "[-0.03438271 -0.02138843 -0.02720902 -0.0353727   0.11102529 -0.05124381\n  0.03785394 -0.08311149  0.00455428  0.01216478]": "Right now the program for ObsidianAssist expects the files to be name 'Life Todo",
    "[-0.05182229  0.06197164  0.02595834 -0.069635    0.03986718  0.00477291\n  0.1088737  -0.01546824 -0.0428478   0.01150687]": "md' and 'Work Todo",
    "[ 0.01659368 -0.01150668 -0.11266833 -0.09505583  0.0729894  -0.05289659\n  0.0212043   0.07510854  0.03121368  0.0301101 ]": "md', and to be placed in the root folder",
    "[-0.11466631  0.05293498 -0.00842198 -0.01297364  0.03453702 -0.02072814\n -0.05314809  0.02933867  0.00510621  0.00332519]": "Maybe extend it to be more custom in the future",
    "[ 0.00511394  0.06019113 -0.03963499 -0.09102406 -0.02454061  0.05176683\n -0.02275183  0.01609308 -0.01774245  0.0185015 ]": "Padding in the ChessTokenizer class can be improved by getting rid of too lengthy games, maybe implement this later",
    "[ 9.5527634e-02 -5.4159682e-02 -3.4127501e-03 -4.8571214e-02\n -1.9189214e-02 -2.2720346e-02 -7.4817874e-02 -1.2898338e-02\n -6.1279058e-02  2.2732998e-05]": "Remember to add some data exploration in the EloGuesser project, just to show some histograms and other useful plots",
    "[-0.06958833  0.00969195 -0.01951439 -0.00864747 -0.07950117 -0.04746599\n  0.03880186  0.01166426  0.00543419 -0.00694886]": "Maybe I can create a dict to create the special tokens in order for the user to change them freely (this is probably unnecessary)",
    "[-0.03101496  0.03294233  0.04704569 -0.03144636  0.0999747   0.06150879\n  0.13668352 -0.0119517  -0.02843643 -0.00455399]": "# 12/07/24\n### Things Todo:\n- [ ] Finish EloGuessr",
    "[ 0.00286799  0.01897575  0.00291657 -0.02876111  0.05863679 -0.08377305\n  0.08481737 -0.05335908 -0.02310466 -0.04784133]": "- [ ] Make ObsidianAssist more modular",
    "[ 0.00976809 -0.1224234  -0.08632681 -0.05362974  0.06590303 -0.04122139\n  0.00971365 -0.06280639  0.02017458  0.00273708]": "- [ ] Create github repos for both EloGuessr and ObsidianAssist",
    "[-0.00744906  0.01996702  0.06133692  0.00534181  0.04435504 -0.00082343\n  0.13878816  0.03100327  0.05802165 -0.05320413]": "### Thoughts of the day:",
    "[ 0.00545411  0.06623449  0.03453817 -0.11552203  0.08628825  0.03975007\n  0.01165547 -0.06231581  0.00939566  0.06413573]": "Remove games that are too short, because it is impossible they ended by check mate",
    "[ 0.10763028 -0.02164375 -0.07619076 -0.09514353  0.0006892   0.06791501\n  0.05528652  0.01209163  0.04809602  0.10441212]": "The minimum amount of moves to checkmate is 4 (2 moves by each player)",
    "[-0.02027709  0.00346953 -0.00289856 -0.02189373 -0.01820952  0.03832832\n  0.04679362  0.08979952  0.03589139  0.05839859]": "This means the minimum amount of tokens in a match is 7, anything below that is a faulty game",
    "[-0.01176547  0.00419285  0.08287086 -0.04203897  0.03017383 -0.06763529\n  0.04175167 -0.0380417   0.0214125   0.08992354]": "Remember to check the syntax error for \\d in the cleaning function for EloGuessr",
    "[-0.00427133 -0.06069458 -0.07203051  0.00289399  0.0715078  -0.02360584\n -0.06572652  0.03486685 -0.03089336  0.01634642]": "Remember to plot some stuff for the EloGuessr data, just to include in the github",
    "[-0.00305008 -0.11011809 -0.10771611 -0.0036829   0.07744893 -0.00722206\n -0.009789   -0.02376361  0.03035633  0.04189618]": "Remember to create the github repo for the EloGuessr",
    "[-0.05966384 -0.00704916 -0.02040547 -0.01974641  0.08852048 -0.04848851\n  0.05145728  0.00152906 -0.02141572  0.00608849]": "Right now the program expects the files to be name 'Life Todo",
    "[-3.4300383e-02  6.5627135e-03 -3.9275743e-02  5.3114425e-05\n  1.9920373e-02 -7.4313246e-02  3.5557795e-02 -8.2905799e-02\n -8.0055438e-02 -2.9907962e-03]": "(This needs some work)\n\n# 13/07/24\n\n### Things Todo:\n- [ ] Establish a chunking routine\n- [ ] Establish a model for encoding chunks\n- [ ] Establish a RAG routine for IR on the encoded chunks\n- [ ] Use NUMPY ONLY FOR THE VECTOR DB FORGET OUTSIDE THINGS\n\n# 1",
    "[-0.02302232 -0.03589467 -0.0167728  -0.01677617 -0.03741489  0.03716157\n  0.04804681  0.015682   -0.03039413 -0.07242785]": "- [ ] Create the initialization of the embedding thing",
    "[-0.05464594  0.0474118   0.03755639 -0.03603756  0.021465   -0.04978519\n  0.05409739  0.01831184 -0.00237633 -0.02677544]": "- [ ] Remember to improve the chunking process when a text split on a period has len > token_limit",
    "[-0.03226699  0.02201559  0.06855667 -0.0385634   0.04370141 -0.03566124\n -0.08023749 -0.02749501  0.00837187 -0.03871816]": "One way to deal with this is to embed the sub_chunks individually but have them point to the whole passage later",
    "[-0.04352842  0.06497608  0.09955661 -0.03266767  0.06432491 -0.01275373\n  0.05834965 -0.02474008  0.01690674  0.03530988]": "(For example, if passage split on '",
    "[-0.0399151   0.05342627 -0.06199744 -0.04243384 -0.05465815 -0.03483962\n  0.05381338 -0.01214529  0.0169449  -0.08870928]": "' is divided into pass1, pass2, pass3, we generate emb(pass1), emb(pass2), emb(pass3), but if any of these appear on the topk of the retrieval procedure we return pass1",
    "[-0.019745   -0.00262373 -0.06998692  0.00944697  0.00708872 -0.04012597\n  0.08907081 -0.0038734  -0.00604301 -0.09164724]": "pass2",
    "[ 0.00242797 -0.09473488 -0.05965994 -0.03204589 -0.02858747  0.08962712\n  0.04611872 -0.06179828  0.02719192 -0.06123894]": "pass3 to the language model)",
    "[-0.07379076  0.09931538  0.03629507  0.02960015 -0.12685485  0.01982064\n  0.06611519 -0.02201882 -0.06361901 -0.01792849]": "- [ ] Look into query rewriting for the RAG functionality later",
    "[ 0.01165486  0.09982044  0.00731785 -0.01079097  0.06424072 -0.05581464\n  0.01538342 -0.0244036  -0.06526079 -0.01968422]": "- [ ] def extract_gmorning_content(self, file_text, num_previous_days: int) -> str add a check to this function to see if the content of the file is in the correct format",
    "[-0.00734135  0.04363662  0.04745802 -0.03630101  0.06293521 -0.02772071\n  0.17261253  0.02169076  0.08994941 -0.07504591]": "- [ ] \n\n### Thoughts of the day",
    "[ 0.0087545  -0.03860682 -0.14977129  0.02408338  0.02567812 -0.02895674\n  0.02185343  0.00185063  0.0055228  -0.00366353]": "We use data collected from Lichess' open database ([https://database",
    "[-0.04765902 -0.05151029 -0.05215935 -0.03771361  0.03786973  0.04031208\n  0.11855085  0.04095347  0.06739011  0.00401351]": "lichess",
    "[ 0.02263361 -0.07891645 -0.04706186  0.04449853 -0.05959981  0.00274565\n  0.02253348 -0.02445409  0.04367137  0.03447085]": "org/](https://database",
    "[ 0.01088798 -0.09183776 -0.10953701 -0.04353876  0.02422538  0.00722061\n -0.03291026  0.02806451 -0.09695438  0.0613076 ]": "org/)) and, for quick access to high rated ELO games, we used the Lichess Elite Database ([https://database",
    "[-0.07054166 -0.04584793  0.0173468  -0.03647915 -0.0852534  -0.00693383\n  0.11132319  0.05361069  0.03600664 -0.00232706]": "nikonoel",
    "[-0.0254685  -0.02572056 -0.04997049 -0.01000483 -0.04538424  0.0332354\n  0.06653252  0.05210197  0.07091886  0.02927857]": "fr/](https://database",
    "[-0.09773885  0.05425557 -0.05943828 -0.06953391 -0.02031856  0.03681493\n  0.07596714  0.04206947  0.02783565  0.06974685]": "fr/)) compiled by Niko Noel",
    "[ 0.03219682  0.01112211 -0.04746295  0.07903264  0.02676349  0.10800949\n  0.12577108  0.04766952  0.03555399  0.01802195]": "add this text to readme",
    "[-0.00214751  0.01498444 -0.10687175  0.00741718  0.11102791 -0.02451962\n  0.02009013  0.0670412   0.02004336  0.01338311]": "txt file on EloGuessrs github page",
    "[-0.10650707  0.02373743  0.0919477  -0.01891041  0.12158273  0.03676262\n  0.18541162 -0.03729338 -0.0330856  -0.05716649]": "# 10/07/24\n### Things Todo\n\n- [x] God damn work",
    "[-0.05935736  0.05584462  0.01920111 -0.08776734  0.08173604 -0.0088242\n  0.1792434  -0.03969971  0.05430547 -0.0991705 ]": "- [x] God damn work-life balance\n### Thoughts of the day",
    "[ 0.02092809  0.03493883  0.04050152  0.00423473  0.01486176 -0.02610119\n  0.00584536 -0.05214176  0.04233873  0.00873436]": "Today I didn't really think about anything, I just kind of floated around town, reading a book by the water and looking at a confederacy of bird marauders consisting of seagulls, pigeons and sparrows try to steal a kebab from a couple sitting right next to",
    "[-0.09239608  0.03280539  0.07281084 -0.02371589  0.17385258  0.02933897\n  0.13807467 -0.02228644 -0.0867771  -0.01481079]": "# 11/07/24\n### Things todo\n- [ ] Finish the Guess the ELO project",
    "[-0.00121645 -0.00339708  0.05140291 -0.11149122  0.00997749 -0.05278543\n  0.14620842  0.0096924  -0.01274679 -0.0325969 ]": "- [x] Have a zoom call with my friends in Brazil\n### Thoughts of the day",
    "[ 0.02032767 -0.0798128  -0.03080717  0.00543614 -0.02671528 -0.05521074\n  0.06255981 -0.03262531  0.11475006  0.02766582]": "I am thinking about different things",
    "[-0.01834073 -0.08493107 -0.01052127 -0.02042812  0.02258401  0.0171147\n  0.06679863  0.03623112  0.05552689 -0.0285766 ]": "I am sitting here",
    "[-0.03227448  0.09753393  0.02368722 -0.01647852  0.08704855  0.01966124\n  0.05769522  0.01642653 -0.05763407  0.01428422]": "# 12/07/24\n\n### Things Todo:\n- [ ] Inquire about the Norwegian Society exam",
    "[-0.02532683 -0.04381987  0.05039107  0.00398743 -0.01840536 -0.04425633\n  0.09192821 -0.04078909 -0.00606174 -0.0701211 ]": "- [ ] Start going to the gym",
    "[ 0.0100376   0.07068053 -0.00854598 -0.03059438 -0.07572092  0.05025515\n  0.12207909 -0.02213543 -0.00576374 -0.05175292]": "- [ ] Help Dasha book her hotel",
    "[-0.0214051   0.01117777  0.05741495  0.00168005  0.02972174 -0.04208037\n  0.07472557  0.00317463 -0.08019078 -0.03551984]": "- [ ] Hike accross Vidden tomorrow",
    "[-0.08627054  0.03009135 -0.10757449 -0.00348553 -0.08352638 -0.00932911\n  0.04652717  0.02717783 -0.0069021   0.04579621]": "Da ponta da faca pra baixo \u00e9 tudo l\u00e2mina, n\u00e3o existe isso de cabo",
    "[ 0.0220489   0.07397548 -0.03799761  0.01727613 -0.01635368 -0.00256103\n  0.01131134  0.0521446  -0.06478588  0.02990829]": "# 1945 - Nascimento de Inoc\u00eancio\n\n\n# 1950 - Intentona Militar\n#### Abril - A capital \u00e9 cercada pelo Ex\u00e9rcito",
    "[ 0.0221056   0.06630167 -0.05298918 -0.03862789 -0.08961232 -0.04227621\n -0.00876093 -0.00432475  0.02696124  0.0872219 ]": "As divis\u00f5es mecanizadas e de artilharia juntam-se ao motim e apoiam o levante monarquista",
    "[ 0.02330273  0.03703066 -0.04290928  0.03308612 -0.10852706 -0.00243548\n -0.02387765  0.02901081 -0.02322016  0.05187521]": "As lagartas dos tanques rolam pelas ruas e a popula\u00e7\u00e3o comum se refugia dentro de casa e n\u00e3o reage",
    "[ 0.08574771  0.02158422 -0.03295565 -0.01435531  0.00936252 -0.03541235\n -0.06687065  0.04094069 -0.03416298  0.00724171]": "Homens fardados e armados com fuzis rondam os bairros, mantendo a ordem e cercando o Pal\u00e1cio do Governo no Distrito Central, bem como o Tribunal de Justi\u00e7a e a Faculdade de Direito",
    "[-0.04812902  0.11759164 -0.03927219 -0.05502812 -0.12382708  0.06857569\n -0.01916578  0.06482387 -0.01883381  0.04655617]": "A pol\u00edcia rende-se aos golpistas e a coroa\u00e7\u00e3o do Rei-Mirim Jo\u00e3o Peixoto \u00e9 programada",
    "[ 0.01314877  0.01608768 -0.06506077 -0.03447712 -0.04706843 -0.05619969\n  0.01692424  0.03531735 -0.03768196  0.06230262]": "As r\u00e1dios oficiais aderem ao movimento golpista, e na televis\u00e3o os rep\u00f3rteres explicam a mudan\u00e7a como um basta aos desmandos do governo republicano",
    "[-0.00115229  0.01855764 -0.01317923 -0.0104301  -0.0897727  -0.01735864\n  0.02684478  0.04156551  0.01093977  0.06162922]": "Uma monarquia constitucional dar\u00e1 lugar \u00e0 antiga ordem",
    "[-0.05315987  0.04871887  0.06185133 -0.01797352 -0.0461592   0.01406889\n  0.10358546  0.03692988  0.00838149  0.05984344]": "### Maio - A coroa\u00e7\u00e3o do Rei-Mirim\nA Fam\u00edlia do Rei se muda de suas terras na Beira-Mar \u00e0 Encantada, a resid\u00eancia oficial do presidente da rep\u00fablica",
    "[ 0.01859165  0.0750828  -0.10729099 -0.10435497 -0.12174122  0.06450766\n  0.00646029  0.01373452 -0.04705347  0.06982261]": "No dia 10 de Maio haver\u00e1 a cerim\u00f4nia de coroa\u00e7\u00e3o oficial, e a Nova Constitui\u00e7\u00e3o, redigida com ajuda de um grupo de juristas alinhados aos militares ser\u00e1 promulgada pela Tribuna Constitucional em uma solenidade",
    "[ 0.02816357  0.04316234 -0.07900728 -0.01030718 -0.00627429  0.02228763\n -0.00534356 -0.01668548 -0.00765969  0.045797  ]": "9 de Maio: o palco est\u00e1 sendo montado no Marco Central da cidade e os preparativos para a coroa\u00e7\u00e3o \n\nApesar da nova denomina\u00e7\u00e3o oficial ser \"Cidade-Estado Livre e Soberana de \"\n\n# 1951 - A Separa\u00e7\u00e3o de Direito\n### A Comitiva do Pa\u00eds Estrangeiro chega \u00e0s Ci",
    "[ 0.01078205  0.09985434 -0.01603591 -0.06975788 -0.08890632  0.03867725\n  0.00605259  0.05716295 -0.03129767  0.05281416]": "Depois da tentativa de golpe e revolta militar, o Presidente do Pa\u00eds Estrangeiro chega com uma comitiva para apaziguar os \u00e2nimos e retomar a ordem local",
    "[-0.04819132  0.09962502  0.02247692 -0.00212018  0.03228538 -0.00082067\n  0.02903685  0.03264008 -0.10357326  0.02561068]": "Os militares, \n\n# 1980 - In\u00edcio",
    "[ 0.02856698 -0.00462324 -0.05514148 -0.01200716  0.0919271  -0.06843737\n  0.02666525 -0.02668014 -0.07052832  0.05195893]": "A common task in logistic regression is binary classification",
    "[-0.01882112  0.02019521 -0.0296987   0.00427216 -0.02149534 -0.06046148\n  0.07229716 -0.08113946  0.01283397 -0.04368411]": "It is the \"Hello, world!\" of connectionism",
    "[ 0.04493796 -0.01849543 -0.12424838 -0.06476197  0.04142452  0.04769989\n -0.01351418  0.02413873 -0.04312538 -0.06899772]": "In order to achieve your objective, you need to feed the computer with labeled examples",
    "[ 0.05527014 -0.03110908 -0.07237905 -0.06120541 -0.00882854 -0.01116646\n -0.04880551 -0.0218282  -0.03496475 -0.00917657]": "It doesn't matter that you need thousands of them, \n\nIn binary classification, you would have two types of data: training data and test data",
    "[-0.0547888  -0.05225876 -0.03482058 -0.01345578  0.01876975  0.04629385\n -0.0116002  -0.0369371  -0.09744752 -0.03815146]": "The training data is what will be fed to the algorithm, it will guide the learning procedure and help fine tune all of those artificial neurons you've heard so much about",
    "[-0.04534948 -0.01078239 -0.07341489  0.03892383  0.03012316 -0.05685633\n -0.05310602  0.00378068 -0.02955591  0.01849625]": "The test data, on the other hand, will be our benchmark",
    "[-0.03940366 -0.0431874   0.03833109  0.01399234  0.10732667 -0.07977002\n -0.08180904 -0.09219977 -0.01819182 -0.0176923 ]": "After training our machine learning algorithm, we test with completely new images and see how well it does",
    "[ 0.00255128  0.06122287  0.00919237  0.03857877 -0.15954182  0.00054128\n  0.0605922  -0.00276448  0.07971397  0.07883351]": "Think of it as an exercise in how you'd teach arithmetic to a child",
    "[-0.01229213  0.03225477  0.01738247  0.03774719  0.03786133  0.00198979\n  0.06680581  0.02131451  0.00378556 -0.0001161 ]": "After providing a few examples \"2",
    "[ 0.09427027  0.05503354  0.02626867  0.03416104 -0.07888769 -0.06871571\n  0.03635897 -0.00672149  0.0248677   0.04610439]": "2=4 is true\", \"2",
    "[-0.02348347 -0.01084007  0.01161339  0.02183653 -0.015774   -0.05392699\n  0.1019035  -0.07760853  0.03041963  0.08985198]": "3 = 5 is true\", '1",
    "[ 0.02796599  0.0498427  -0.01454582  0.01612245 -0.13201119 -0.0876791\n  0.03043399  0.07804288  0.00852739  0.03228265]": "1=2', you might ask the child if she or he has understood the operation",
    "[-0.01812941  0.04584559 -0.04977415 -0.02406284 -0.01835173 -0.00432916\n  0.0715452  -0.01517162  0.04426052  0.07620432]": "Then, in order to perform a check, you present the child with new numbers in the addition",
    "[-0.00157455 -0.06377426 -0.00096662 -0.01718036  0.09202494 -0.04146127\n -0.05352983 -0.06357031 -0.04759856 -0.06989472]": "As Yann LeCun is quick to remark, if you want a machine learning algorithm to be able to classify cows but in all of your examples the pictures of cows were taken in brightly lit, emerald green meadows, the algorithm might pick up on the green background a",
    "[0.01340538 0.05661521 0.02453944 0.05490488 0.10285833 0.08440802\n 0.08040973 0.06129733 0.07269432 0.03090811]": "In the famous opening paragraph of the Philosophical Investigations, Wittgenstein lays out a long quote from St",
    "[-0.02227629  0.05849667 -0.03584478  0.04700851 -0.01680757  0.04277581\n  0.07901238 -0.0184167   0.07145017 -0.03537975]": "Augustine's *Confessions*, in which the theologian describes how he and other children might acquire language:\n\n\"When grown-ups named some object and at the same time turned towards it, I perceived this, and I grasped that the thing was signified by the so",
    "[-0.09281047  0.02672708  0.02410233 -0.02818247  0.03175239 -0.04642264\n  0.16641662  0.00504862 -0.02884747 -0.05215112]": "[",
    "[ 0.1285001   0.07598482  0.01504221  0.01148638  0.02751087  0.02063047\n  0.12716316 -0.01074506 -0.04273708  0.036341  ]": "] In this way, little by little, I learnt to understand what things the words, which I heard uttered in their respective places in various sentences, signified",
    "[-0.00714559 -0.01472818  0.08364835  0.0701875  -0.07141241 -0.06367832\n  0.09946012  0.02503508  0.13081074 -0.10596953]": "And once I got my tongue around these signs, I used them to express my wishes",
    "[-0.08760214 -0.00773975 -0.01628442  0.02801534 -0.06743861  0.00138159\n  0.04315854  0.02661399  0.10115198 -0.01749258]": "\" (PI, 1)\n\nIt is a remarkably intuitive idea",
    "[ 0.00726348  0.01326443 -0.01604042  0.03964299 -0.08427591 -0.01203933\n  0.10317359 -0.02916861  0.10713176 -0.01211736]": "An adult points to an object, utters a word, and the child somehow connects word to world and is able to pick it out in the future in order to communicate with other human beings",
    "[ 0.00325213  0.04356721 -0.01921718  0.00344883 -0.02601177 -0.02142419\n -0.05270346 -0.01732296  0.09347905  0.03018912]": "Much of the Philosophical Investigations is meant to show how this image (the Augustinian picture of language acquisition) is incredibly incomplete and lacks explanatory power",
    "[ 0.03541334  0.01335568  0.05938839  0.04285961  0.04963433 -0.01434495\n  0.04768274  0.03310635  0.05513459  0.05003384]": "But Wittgenstein is not saying Augustine was wrong (after all, he was just making a passing observation, not trying to build a theory), he just wants us to not be deceived by this picture",
    "[-0.00830385 -0.08173487  0.07570751  0.07936899  0.08055958 -0.02143279\n -0.04369615 -0.06497612 -0.02249561 -0.02231915]": "Let's go back to our machine learning example from the beginning, the one with the cows in green pastures",
    "[-0.01753552 -0.0482006  -0.00783359  0.03339159  0.03370921  0.05331092\n -0.05455884 -0.04817224 -0.03617143 -0.04275218]": "How \n\nWhat is supervised learning if not a large engine operating within the confines of the augustinian picture of language? \n\nThink about it",
    "[-0.0418938   0.0662408  -0.08785258 -0.04859648  0.03242806  0.0330302\n  0.02896392  0.05766115  0.08174821 -0.05316844]": "We give a computer label-picture pairs, where the label stands in for the word and the picture for the world",
    "[-0.0061821  -0.0093655  -0.02217805 -0.0585282  -0.00147348 -0.00059732\n  0.07346667  0.0177888   0.08753    -0.02200925]": "This is our way of pointing something to a machine",
    "[-0.00493721  0.06948101 -0.00459284 -0.1046234  -0.00651478 -0.11231721\n -0.00297563 -0.03527324  0.0815509  -0.03396863]": "A computer picture, after all, is composed of a finite number of pixels, each with a little difference in color",
    "[-0.02744567  0.02673036 -0.0292519  -0.0046023   0.12189176 -0.04426168\n -0.0196409   0.00399733  0.03494705 -0.01843317]": "Photographs are obviously not stand-ins for real vision",
    "[ 0.08046091 -0.00690071  0.02103729 -0.03574242  0.1113077  -0.04491273\n -0.09453931  0.00075321  0.05336815 -0.02191033]": "Just think of the difference between seeing pictures of famous architectural landmarks and actually being on site",
    "[ 0.06531833  0.02965422 -0.02430937  0.00177584  0.15122864 -0.0764564\n -0.05302125 -0.00963016  0.05966365 -0.06902312]": "The experience is radically different for many reasons, and one of them is that in real life we lack the *framing* of photographs",
    "[ 0.06374809 -0.03874061 -0.04631076 -0.09152972  0.08291243 -0.04518755\n  0.05973742  0.05388883  0.06912754  0.0498765 ]": "We have peripheral vision, we cannot zoom into details",
    "[ 0.03183621  0.05630146 -0.07121624  0.0233802  -0.00099941  0.09299508\n -0.0396981  -0.03292666  0.08567519 -0.09525309]": "Our field of view is finite, but not flattened into a rectangular representation",
    "[ 0.05118651  0.02278604  0.04835453 -0.07109705  0.04355333 -0.01099567\n  0.03366168  0.00322958  0.03712798 -0.00259933]": "### Lebensform, grammar, language games \n\n\nLebensform perhaps does not fit so well with the picture I've painted so far, and it is natural that it doesn't since the concept has eluded exegesis and it appears only sparsely in his writing",
    "[-0.0132504  -0.04048076  0.06181219 -0.06237543  0.00777992  0.09881776\n  0.04142095  0.03990794  0.02805482  0.05241921]": "It is, nonetheless, a central idea that needs to be taken seriously bu\n\n\n### Scenarios in RL Scenarios as a \"complete language\"\nAfter describing the first language game (LG1) in the book, let's call it the Slab-Game, Wittgenstein makes a puzzling plea:  \"C",
    "[-0.02143805  0.05174653 -0.04276274  0.05053813 -0.01782586  0.06163191\n  0.02260412  0.00520576  0.02360127 -0.02142047]": "\"\n\nFor those of you who have never read the Philosophical Investigations (and for those who need a refresher), the book begins with a long quote from Augustine's _Confessions_ in which the theologian describes how he learned how to speak",
    "[ 0.05837941  0.03900992  0.01389865  0.07735023 -0.07637878 -0.00125997\n  0.11342261 -0.00761478  0.04098017 -0.04014768]": "He says that when adults turned towards an object and uttered a word, thereby \"naming\" it, he slowly learned what the different words signified and could refer to them with ease in the future",
    "[-0.00400823 -0.02585985 -0.05575748  0.05168262 -0.00214554 -0.00655424\n  0.03783207  0.08925474  0.04626738  0.02099549]": "Under this model, you have an object in one extremity and a word in the other",
    "[ 0.00621505 -0.06563521 -0.10956869  0.04871065 -0.11671394  0.02367665\n  0.09987982  0.01186749  0.086297    0.0018501 ]": "You point to an object, say a word, and create a connection between word and object",
    "[-0.00663237 -0.02406606 -0.00998233 -0.01019568 -0.15371296  0.01653594\n  0.09096038  0.01237596  0.14623071 -0.02937536]": "It is this connection that we call meaning when we say \"A means X\"",
    "[-0.00228576  0.00860537 -0.03842316  0.03827436 -0.0285018   0.01320637\n -0.06243492  0.0292825   0.00974794  0.06739707]": "Now, it would be absurd to say Augustine was putting forward a philosophical (much less a scientific) theory",
    "[ 0.073505    0.05839757  0.02304217  0.01981882  0.02566813  0.03220046\n  0.04432245  0.00249802  0.0130864  -0.05522014]": "After all, he was just making passing remarks on his life experience and not trying to offer a full blown explanation of language acquisition",
    "[ 0.04059436 -0.00808341  0.01210915  0.01756552 -0.00529296  0.02515448\n  0.01826424  0.01290767  0.08584512  0.02350841]": "Wittgenstein knows this, and instead of arguing against it or trying to refute it, he says \n\nIf that's the case, why take Augustine's words seriously?\n\nThe fact of the matter is that the Augustinian Picture of language is extremely intuitive even though we",
    "[-0.01024468  0.03178873  0.02431715 -0.02996694 -0.01436651  0.02860221\n  0.05670535  0.00732507  0.04312934  0.00654602]": "It's the kind of view we ourselves could very easily come up with should a child ask us for a definition of language",
    "[ 0.05313437  0.00097406 -0.01253653 -0.07267056 -0.0449188  -0.00135724\n  0.15821007  0.02363599  0.16070335 -0.05036163]": "When we say that we use language for communicating thoughts",
    "[-0.00219514  0.04309543 -0.03741416  0.00348337  0.00895823  0.03011375\n -0.03661603 -0.00556414  0.00919765  0.0102277 ]": "In other words: the Augustinian Picture is self-persuasive in a way that can be hard to see past",
    "[-0.00281717 -0.03879163 -0.01260114 -0.02570027 -0.14089231 -0.01930542\n  0.07231107 -0.00183768  0.07051998  0.00456224]": "It is easy to satisfy ourselves of its correctness without a second thought",
    "[-0.04740617  0.00505891  0.00531638 -0.00158274 -0.02643324 -0.01363126\n  0.04156816  0.02561767  0.05007635  0.0002553 ]": "One can say one of Wittgenstein's projects in the PI is breaking the spell of the Augustinian Picture",
    "[ 0.0243969   0.10058259  0.05831243  0.03590438  0.03214112 -0.04484318\n  0.1172021   0.0193532   0.00941552 -0.03550079]": "Notice I did not say that he wants to refute it",
    "[ 0.02986601  0.01138918  0.01011708 -0.08265823 -0.02812352  0.04991897\n -0.04156259  0.02413199  0.01991133  0.08004173]": "With this in mind, let's go back to section 2 of the Philosophical Investigations and explore the ways in which the Slab-Game might be conceived as a complete primitive language",
    "[ 0.0154774   0.06867272  0.02727459 -0.01829008 -0.00463306  0.07105764\n  0.0543878  -0.02302983  0.0624515  -0.02434374]": "For fear of leaving something out, let me quote directly from the book so we all have the exact same fragment for consideration:\n\n- \"Let us imagine a language for which the description given by Augustine is right: the language is meant to serve for communi",
    "[ 0.00709901  0.0699081   0.00784182  0.03712915 -0.04807641  0.01961373\n -0.01525612 -0.02537661  0.04230683  0.05173869]": "A is building with building stones: there are blocks, pillars, slabs and beams",
    "[ 0.01615158  0.05610073 -0.01471145 -0.0494261  -0.08406092  0.06315876\n  0.06940334 -0.05830299 -0.00289145  0.02222018]": "B has to pass him the stones and to do so in the order in which A needs them",
    "[-0.0149404   0.05794785 -0.03392208 -0.03043257 -0.0370903  -0.02178301\n  0.02310595  0.00034318  0.06756384 -0.01198439]": "For this purpose they make use of a language consisting of the words \u201cblock\u201d, \u201cpillar\u201d, \u201cslab\u201d, \u201cbeam\u201d",
    "[-0.0598436   0.08917052 -0.06303339  0.00096032 -0.07674427 -0.0136168\n  0.12566613 -0.00516649  0.13842052 -0.03104267]": "A calls them out",
    "[ 0.01302089  0.10415512 -0.01668144 -0.03942328 -0.07745687  0.07217231\n  0.14071931 -0.05113612  0.00946667 -0.02740698]": "B brings the stone which he has learnt to bring at such-and-such a call",
    "[ 0.00557197  0.06984241  0.05855146  0.00545857 -0.06281945 -0.02269688\n  0.06083111 -0.00919182 -0.04054957  0.029648  ]": "\"\n\nIn what way may I conceive of this as a **complete primitive language**? Let's start with the simple part",
    "[ 0.00632501  0.00926629 -0.02879071 -0.00649837 -0.03961128 -0.00930881\n -0.01290626 -0.01124202  0.04400935 -0.0159035 ]": "That there is a language in the example is easy enough to see: the language contains four words, namely, \"block\", \"pillar\", \"slab\", and \"beam\"",
    "[ 0.01444864  0.00959307 -0.01579073 -0.08888207 -0.07835434  0.02557449\n  0.03385054 -0.01927608  0.03366264  0.07420821]": "The participants of the game, builder A and assistant B, understand these as words and their use is not distant from ours",
    "[ 0.02550871  0.02776817 -0.06000723 -0.05591492 -0.06978161 -0.01274197\n  0.10220412 -0.03818959  0.12361857 -0.02982181]": "They are communicating thoughts to each other and using them to achieve a certain task",
    "[ 0.08331419  0.00461716  0.05339426 -0.02434976 -0.01036049  0.03271952\n  0.06586894  0.07835709  0.03686821  0.05532431]": "But how exactly is it primitive? \n\n\n\n### Language games, grammar and lifeform\n\nI obtained this powerful insight through Juliet Floyd's comments about Turing \n\n# Wittgenstein and AI\n\nThe real question here is this: how are we to understand \"meaning is use\" ",
    "[ 0.04915687 -0.02033848  0.02422746  0.01179823 -0.00215723 -0.00923147\n  0.15700974  0.09424684  0.13635045  0.04681959]": "Remember, withing this probabilistic view of meaning, the meaning of a word is determined by\n\nWittgenstein's main line of attack in the PI where a *proposition* acts as an intermediary between words and objects",
    "[ 0.00242379 -0.00812437 -0.04158937  0.00587217 -0.08805504  0.03263628\n  0.01649086  0.06445712  0.02849354 -0.03345115]": "________________________________________________________________________\nPI 108: We see that what we call \"sentence\" and \"language\" has not the formal unity that I imagined, but is the family of structures more or less related to one another",
    "[ 0.00830364  0.02625849 -0.00705622 -0.00725209  0.06690535 -0.00566937\n -0.00474105 -0.01770446  0.10933382  0.07559796]": "-- But what becomes of logic now? Its rigor seems to be giving way here",
    "[ 0.07287546  0.03093694 -0.01564388 -0.01458362  0.06345396  0.05892764\n  0.00210167 -0.03313923  0.12687317  0.08397547]": "-- But in that case doesn't logic altogether disappear? -- For how can it lose its rigor? Of course not by our bargaining any of its rigor out of it",
    "[-0.06265016 -0.00237063  0.02665757  0.0423652   0.00515252 -0.04775015\n  0.02874317  0.00933094  0.01412913  0.02711622]": "-- The _preconceired_ idea of crystalline purity can only be removed by turning our whole examination around",
    "[ 0.00231045  0.01579903 -0.05994064 -0.04335185 -0.03867864  0.02919312\n -0.01460941  0.00484915  0.09233709  0.04712508]": "(One might say: the axis of reference of our examination must be rotated, but about the fixed point of our real need",
    "[ 0.11656933  0.03364102 -0.02922072 -0.00697411 -0.00287895  0.03619863\n  0.10282601 -0.00163429  0.09400579  0.05135856]": ")\n\nThe philosophy of logic speaks of sentences and words in exactly the sense in which we speak of them in ordinary life when we say e",
    "[-0.08668178 -0.00471964  0.01585597  0.02402103  0.00147646 -0.05535593\n  0.1287492   0.02936236 -0.03466741 -0.0122676 ]": "g",
    "[-0.000379    0.07590687  0.05018425  0.04166634  0.00376325  0.06197979\n  0.0405394  -0.0869799   0.08851299 -0.01334725]": "\"Here is a Chinese sentence\", or \"No, that only looks like writing",
    "[-0.02696027  0.02959386 -0.00464399 -0.00357269 -0.07968522 -0.02591483\n  0.07795832 -0.05279166  0.05296007 -0.03015606]": "it is actually just an ornament\" and so on",
    "[ 0.07094355 -0.03218592  0.02770627 -0.01189344 -0.05650519  0.0482279\n -0.02176671 -0.08400095  0.11744601  0.03236121]": "We are talking about the spatial and temporal phenomenon of language, not about some non -- spatial, non -- temporal phantasm",
    "[ 0.00920814  0.02011619  0.02565305  0.06406803  0.05927891 -0.01895154\n  0.06213383  0.01131126  0.05192855 -0.0245917 ]": "[Note in margin: Only it is possible to be interested in a phenomenon in a variety of ways]",
    "[ 0.06596822  0.04994149 -0.04783847 -0.05725998 -0.00522315 -0.06180992\n  0.02394428  0.01085751  0.08214462  0.01243283]": "But we talk about it as we do about the pieces in chess when we are stating the rules of the game, not describing their physical properties",
    "[ 0.01055956  0.03864451 -0.08153256  0.02806992 -0.0527565  -0.00146335\n  0.00097595  0.08029121 -0.01772875  0.08053291]": "The question \"What is a word really?\" is analogous to \"What is a piece in chess?\"\n________________________________________________________________________\n\nPI 109: It was true to say that our considerations could not be scientific ones",
    "[ 0.02799151  0.00395871  0.01704008  0.04008437  0.07099792  0.02148928\n -0.00185904 -0.01747867  0.08576972  0.06397936]": "It was not of any possible interest to us to find out empirically that, contrary to our preconceived ideas, it is possible to think such-and-such -- whatever that may mean",
    "[ 0.01613368 -0.02112698 -0.03773792  0.03637252  0.01087923 -0.03150593\n  0.07651934  0.07565006  0.06209202  0.05188693]": "(The conception of thought as a gaseous medium",
    "[-0.02346296 -0.03611118 -0.01351021  0.04445983  0.03244697 -0.03293062\n -0.03224535 -0.08972712  0.07909366  0.06985942]": ") And we may not advance any kind of theory",
    "[-0.01516998  0.05401614 -0.07741831 -0.0954758   0.02569352 -0.00429411\n -0.02627838 -0.00950659  0.05200067  0.08685228]": "There must not be anything hypothetical in our considerations",
    "[-0.0348491   0.09441085  0.00785201  0.04851025  0.09179503 -0.02084883\n -0.00189182 -0.06474213  0.09744632  0.00480885]": "We must do away with all explanation, and description alone must take its place",
    "[-0.00285218  0.08136176 -0.04491453  0.04373453  0.00926854 -0.00564162\n  0.06185932 -0.01839544  0.06353892  0.00075109]": "And this description gets its light, that is to say its purpose, from the philosophical problems",
    "[ 0.03843715  0.05119074  0.05920826  0.01951373  0.00448772 -0.0141217\n -0.00488764 -0.06055065  0.01426692  0.03495569]": "These are, of course, not empirical problems, they are solved, rather, by looking into the workings of our language, and that in such a way as to make us recognize those workings: in despite of an urge to misunderstand them",
    "[-0.00831631  0.08446056  0.01791057  0.02752906 -0.02556637  0.02389724\n -0.04067599 -0.06575797 -0.01707009  0.06846596]": "The problems are solved, not by giving new information, but by arranging what we have always known",
    "[ 0.03592744  0.07448427 -0.07120665 -0.01706249 -0.03130335  0.07548153\n  0.07842716 -0.05550308  0.02759607  0.02434962]": "Philosophy is a battle against the bewitchment of our intelligence by means of language",
    "[-0.04583918  0.05476511  0.03469534 -0.03522613 -0.00026517 -0.02531743\n -0.06005291 -0.04459496 -0.06633066  0.02625193]": "In May 13, 1891, almost all of the records regarding the ownership, sale and acquisition of slaves were burned in Rio de Janeiro",
    "[ 0.02162059 -0.01823746 -0.08063703  0.05228894 -0.04407633  0.03761073\n -0.03956865  0.05600401  0.03404326  0.0506026 ]": "Ruy Barbosa, then Minister of Economics of the young Brazilian Republic, gave the order",
    "[-0.0708718   0.14112887  0.09225141  0.034587    0.0709231  -0.04151378\n -0.08864488 -0.01175441  0.00862272  0.02200948]": "These documents, which would have offered precious historical insights, were forever lost in a pile of ash and embers",
    "[-0.08256616  0.0808376  -0.06510588  0.0398655  -0.03365964  0.01662375\n  0.02392635 -0.03509096 -0.0116385   0.04148369]": "The names of slaves, their place of birth, \n\nThere are three theories as to why Ruy Barbosa did it: \na) In order to prevent former slave owners from asking indemnizations (essentially, damages) from the State following the abolition of slavery",
    "[-0.0533828   0.0236442  -0.02386338 -0.02380891  0.01729843  0.06239022\n  0.00033163 -0.05131002 -0.05765533  0.08445235]": "b) In order to prevent slaves from asking damages from former slave owners (the abolition in Brazil had a very tortuous and non-linear history, with trafficking being banned in 1850 but slavery continuing as a normal institution until 1888)",
    "[ 0.01514541  0.04976714  0.05248383 -0.01891558  0.05871028  0.04156863\n  0.02515809 -0.04237962 -0.0270543   0.05221839]": "c) In order to erase the shameful past and paint the newly formed Republic in better lights",
    "[-0.03072075  0.02623383 -0.00904542  0.00285052 -0.00553813  0.03734364\n  0.02571488 -0.01498098 -0.13535386  0.07233077]": "These questions are too general, and finding a way of tackling them *in toto* is hard",
    "[ 0.00979232 -0.0298658  -0.03292346 -0.01861365  0.01106332 -0.00104698\n  0.01214931 -0.03227207  0.0271796   0.02696286]": "So let's start with a well defined task, and see if we can think of anything",
    "[-0.01523985 -0.11137933  0.0370039  -0.0528136   0.03543885 -0.01663496\n -0.03411218 -0.03619349  0.02907565 -0.00936455]": "LLMs are here, they are being hyped, they are being integrated into all kinds of online services and they will likely remain in use for years to come",
    "[ 0.1093315   0.00906873 -0.02988623  0.00073635  0.0208725  -0.06655206\n  0.01631545 -0.06553203  0.05379369  0.02464225]": "Yet, they hallucinate",
    "[-0.03986429  0.01734489 -0.03724923  0.03512145 -0.06690811 -0.04335436\n  0.01476982 -0.05284074  0.12042041 -0.00472512]": "That is, they generate strings of text which, to human readers, are deemed to be false",
    "[-0.01329648  0.03843876 -0.03125598  0.01236328 -0.0891149   0.03383813\n -0.01665019 -0.00695316  0.07517666 -0.00266426]": "If you ask it to summarize a book, it will mix up the names of characters and their characteristics",
    "[-0.01329495  0.03981068  0.04645949  0.01346583  0.03460238  0.04068106\n -0.00079816 -0.02781488  0.01254585 -0.02347947]": "If you ask for a famous person's biography, they can tell you their wrong date of birth, place of birth, which university they attended, etc",
    "[ 0.04024541  0.099751   -0.0194729  -0.01107534  0.02048286 -0.00574958\n -0.0030058  -0.05157727 -0.02580474 -0.01057161]": "This has been framed as the \"hallucination problem\"",
    "[ 0.01965218 -0.0054789   0.02593718 -0.03159754 -0.00038837  0.02919659\n  0.03308525  0.00442672 -0.01924102  0.06012405]": "The main question to be considered here is this one: can Wittgenstein help us solve the problem of hallucination in LLMs?\n\nThere is a simply answer here, and it is: no",
    "[ 0.02565763  0.01620613 -0.07666278 -0.00925471  0.00845157 -0.00249777\n  0.02111542 -0.06676151 -0.02755216 -0.00640654]": "Of course he can't",
    "[-0.00829786  0.00232526  0.01514114 -0.02760464 -0.00909279 -0.06056871\n  0.04462979  0.02502797 -0.09909678  0.06441991]": "Good old Witty wasn't a computer science, and his musings from the 30s, 40s and 50s do not directly address the issue",
    "[-0.06054641  0.09438925 -0.02455901 -0.01357288  0.00180034 -0.04691537\n  0.02949476 -0.01118789 -0.0556752   0.0344649 ]": "Moreover, it's not even clear what solving this problem would entail",
    "[ 0.07592371  0.05832425 -0.01742715 -0.05283887 -0.00415627  0.00975857\n  0.03762129  0.02174254  0.05271002 -0.01004325]": "If you're business minded, the problem is a little simpler",
    "[-0.0328631   0.09908295  0.05402363 -0.08441505  0.03466197 -0.02012844\n  0.1313237   0.016583    0.03815311 -0.07980361]": "\"Just make so that my clients don't hate my product when they interact with it",
    "[-0.04440372  0.00037864 -0.02970705 -0.0081047   0.0046425  -0.04319972\n -0.05987309  0.02910384 -0.02547051  0.03455424]": "I can't have my data cleaner AI mess up instructions on how to fill out spreadsheets\", they could say",
    "[ 0.00900856 -0.00388446  0.00036324 -0.04525363  0.06254489 -0.01844963\n -0.03676471  0.0115584  -0.03721122 -0.0151665 ]": "And this is all fine and well",
    "[-0.00067939 -0.07218321 -0.04764147  0.0215453   0.04496749  0.03504458\n -0.07086074 -0.0018704  -0.04816622  0.05311296]": "Within such a realm, it's possible that current approaches such as scaling up and finding ways to refine RLHF might meet some Californian's expectations of intelligence",
    "[ 0.06439386  0.02215103  0.07012769  0.01270831 -0.06420895 -0.02902302\n -0.01042216  0.05204779  0.07826234  0.01612323]": "Yet this bar is too low, and not very interesting",
    "[-0.01607286  0.05228937  0.03794785 -0.01798733 -0.02978685  0.00737589\n -0.01079865 -0.00951309 -0.02698272  0.03457123]": "So we had a question, and we need to break it down into a couple of subquestions",
    "[ 0.11139555 -0.04932263 -0.00674129 -0.01455764  0.02268093  0.01990219\n  0.0382719   0.00459396  0.07448335  0.05261907]": "1: Why do LLMs hallucinate?\n\n\nTechnically minded readers (i",
    "[-0.01528918  0.05226416  0.04121932  0.05939609 -0.03604065 -0.00949968\n  0.13223603  0.03249479  0.02445365  0.00738841]": "e",
    "[-0.04603427 -0.0108623   0.03236737  0.0381876  -0.03467292 -0.07877272\n -0.02748388  0.03595555 -0.01980502 -0.00543556]": "ML researchers and engineers) would have further questions: do",
    "[-0.05420933 -0.03757111  0.00700306  0.03353047  0.01901445  0.00770483\n -0.00983654 -0.04056563 -0.0354176   0.03716543]": "But I would like to pushback on a certain assumption",
    "[-0.09319275 -0.05567231  0.01114154 -0.06719485  0.04654166  0.02138735\n  0.01052921 -0.00322518  0.02262689  0.01307643]": "At one point in the paper, it is claimed that techniques such as word2vec plausibly capture \"the diverse uses of expressions as well as their miscellaneous family resemblances across a plethora of language games\"",
    "[-0.02446163  0.00868027  0.03452062 -0.05000269  0.01454791 -0.04248149\n -0.02776898 -0.01322877  0.14969647  0.02498164]": "I want to understand a bit more why you think this is the case, so I'll first explain the source of my puzzlement and then we can see if we're in agreement or disagreement",
    "[ 0.0138569  -0.1479626   0.03239425 -0.01033416  0.01488597  0.04972063\n -0.00756761 -0.00742119  0.08621977 -0.00671266]": "It seems to me that all word embeddings do is to plausibly capture what is called the distribution hypothesis, that is, the claim that \"linguistic items with similar distributions have similar meanings\"",
    "[-0.04054946 -0.08372812  0.03277136  0.02796774  0.04004735  0.04864658\n -0.076928    0.02828434  0.09034803 -0.01651923]": "What these models do is to look at co-occurence",
    "[ 0.04846774 -0.10253523 -0.02513714 -0.06238521 -0.02169932 -0.01476618\n  0.00547945 -0.07968941  0.02600932 -0.01374086]": "They check which words appear in similar word-neighborhoods, and then assign these words to different locations in a vector space to mirror their distance",
    "[ 0.03349957 -0.05839645 -0.01061571 -0.06856493 -0.08644553  0.05072033\n -0.04330948  0.00341668  0.12466616 -0.02898612]": "For example, it is likely that words like 'ball', 'piece', 'dice' and 'games' appear frequently together, therefore they would be assigned numerical representations in a close interval",
    "[-0.00306611 -0.11321324  0.00671834  0.00392124 -0.05090963 -0.03628334\n  0.00216803 -0.02905839  0.15225275 -0.01154951]": "On the other hand, a word like 'beam' would hardly ever occur in the same neighborhood as 'dice', and thus they would be quite apart in the vector space, signaling their semantic difference",
    "[ 0.02365297 -0.05259792 -0.11148145 -0.06955532 -0.06475669 -0.04678899\n -0.02426589 -0.01741189  0.02882787 -0.01917116]": "There are, of course, several computational techniques to define semantic similarity and other such notions",
    "[-0.01275395  0.00234721 -0.08586331 -0.04103476 -0.04614996 -0.00872432\n  0.01219263 -0.00752786  0.10825582 -0.04459589]": "What they all have in common is this: they only take text as input",
    "[-0.00819817 -0.12720212  0.04585987 -0.01772916  0.04465626 -0.03518463\n  0.01184438  0.01593163  0.09112507 -0.0042729 ]": "Large language models such as GPT-3, which rely on the tokenization of words, live entirely in lexical space",
    "[ 0.00215732  0.03132942 -0.05161869 -0.00674811 -0.10344986  0.0294424\n  0.00512532 -0.04171191  0.06354918  0.00442954]": "Their only layer of access to language is through strings, and through the manipulation of strings using matrix operations",
    "[ 0.00321048 -0.03382362 -0.01743405 -0.05021928 -0.06124445  0.06926347\n  0.11157342  0.03087043  0.03467424 -0.00579975]": "My question here is this: in which way can this be said to mirror the notion of language games and, more broadly, of meaning as use? It seems to me that word2vec spouses, at most, a notion of \"meaning as usage\"",
    "[-8.8505250e-05  1.7295677e-02  2.7022738e-02 -8.3362862e-02\n -1.0447596e-01  9.8424107e-02  9.1652021e-02 -3.8279220e-02\n  7.6692231e-02  5.2915420e-02]": "Its relation to language games such as giving and receiving orders",
    "[ 0.01132259  0.08291442 -0.05121194  0.03861017  0.07207747  0.07154009\n  0.03786783  0.02874237  0.0650259   0.03852648]": "reporting an event",
    "[0.00925109 0.07042564 0.00654675 0.02819753 0.05106558 0.03919963\n 0.02734278 0.0389927  0.0117841  0.06307065]": "forming and testing hypotheses",
    "[ 0.0292795  -0.03064329  0.04264252 -0.01552784  0.00797167  0.02497244\n  0.05888485  0.01450148  0.10312498  0.01432525]": "etc, is at most indirect: it (((experiences))) these things through characters, not through the multimodal interaction of human language games",
    "[ 0.01132477  0.00725078  0.07494605 -0.03912325  0.06023882  0.02222978\n  0.07857762 -0.09594752 -0.05474105 -0.03015888]": "# 1: Art is on its way of becoming a certain kind of infrastructure\n## 1",
    "[ 0.02435307  0.06734636 -0.03034788 -0.08550184 -0.0017758  -0.06407715\n -0.07128193 -0.05223633 -0.04156787 -0.00158015]": "1: Only triangular art can become infrastructure\n## 2: If we wish to resist the infrastructurification of all art, we ought to push for circular art instead of triangular art\n\nArt is becoming something of a infrastructure",
    "[-0.04104521 -0.05283282  0.05260792 -0.027177    0.0024299   0.10294578\n -0.03239912 -0.06495559  0.05572521  0.01340247]": "The shift from the existence of different art forms to \"content\" is a testament to this phenomeon",
    "[ 0.05873036  0.02135883 -0.03232165 -0.01235656  0.00511362 -0.03158668\n  0.13559535 -0.02104548  0.12898897 -0.06293117]": "The bank transfer view of meaning in art: a human being has feelings, and by making a few bodily motions, he or she sends these feelings across the realm of the mind and into the world, thus transfering meaning into an object",
    "[ 0.09102406  0.05520925  0.00861777  0.05476258 -0.03690708 -0.00663559\n  0.10552376 -0.03819964  0.10154288  0.00711064]": "The object is now the depositary of meaning",
    "[ 4.5565270e-02  4.5043573e-02  5.5821992e-02 -1.0485758e-02\n -2.1535937e-02 -1.7745676e-03  4.9476136e-02  2.8311593e-05\n  1.0131488e-01 -3.2589503e-02]": "Wherever the object goes, there too goes \"the meaning\", since it has been indelibly confered by the meaning giver",
    "[ 0.0383262   0.00131229  0.03229668 -0.03081425  0.07245237  0.0234965\n  0.02867568 -0.0587191   0.07884516 -0.03261468]": "Under this view, AI art can never be art because it lacks intentionality, that is, the conscious entity whose feelings are sent out to the world like a promissory note addressed to the audience is nowhere to be found",
    "[-0.08599745 -0.02777353 -0.12349164 -0.04177634 -0.01226627 -0.03728278\n -0.04135185 -0.02498588 -0.02658459 -0.00802072]": "Where we expect an agent there we find only poorly calibrated float32 digits held together by linear algebra and gradient descent",
    "[-0.02460822  0.03280732  0.08662781  0.04141843  0.05339706  0.03509378\n  0.1274516   0.0133306  -0.00881495 -0.05510129]": "## Preface\nThe fact that this text has been written in English ought to be cause for muffled laughter",
    "[ 0.01159988 -0.04033433 -0.02504474 -0.04975646  0.09279637 -0.0657823\n  0.01806921 -0.05751312 -0.02854045  0.054866  ]": "## Infrastructure\nThere used to be a \n\n#### 1\n\n\n\n#### 2 \n\nThe creator-spectator binary is easy to describe",
    "[-0.00818598 -0.03374033 -0.02688155 -0.01323679  0.03816054  0.0533752\n  0.10300273 -0.01992459  0.09073229  0.00284729]": "Someone, the \"artist\", creates a certain work (a movie, a book, a painting, a sculpture, a photograph, a song, a play), and the audience, the \"spectators\", come into touch with the work by focusing their attention on this work for a slice of time",
    "[ 0.03273983 -0.12277655 -0.01259184  0.02145853  0.06146551  0.033891\n  0.03510981 -0.01341101  0.12990442 -0.00065817]": "If you are watching a movie, you might be seated at your couch watching the images unfold before your eyes",
    "[ 0.07642883 -0.04243143  0.02648105 -0.0230419   0.01164155  0.02590268\n  0.02775427 -0.03421682  0.06806968 -0.00460714]": "If you are attending a concert, you might be standing with a crowd while everyone faces the same direction",
    "[ 0.04164032  0.07558224 -0.13006742 -0.08670315 -0.04796796 -0.02123388\n -0.09120394 -0.06067184 -0.02714142 -0.01041144]": "#### 3 Triangular art and circular art\n\nThe nineteenth century inaugurated commercial triangular art, but by no means invented triangular art itself",
    "[ 0.10633165 -0.02003466 -0.02122312  0.001084   -0.05607227  0.12338331\n  0.06209725 -0.01490851 -0.04237578  0.10864738]": "The mass distribution of novels, the erection of cabarets, the invention of circuses, all point to a spectacular accumulation of spectators",
    "[ 0.07125034 -0.02968511  0.05494516 -0.00777793  0.14690964  0.02282161\n -0.04540867 -0.02774424 -0.07660366  0.04738349]": "To gaze upon becomes, in the spectatorial parts of the world, the national past time of the middle and upper class",
    "[ 0.01742906 -0.02183484 -0.04192583  0.00791715  0.00347494  0.01905187\n  0.16975074 -0.02524154  0.04104668  0.03728362]": "To gaze, perchance to see",
    "[ 0.08081046 -0.01757787  0.01448712 -0.01081635 -0.04017646  0.0755157\n  0.03741882 -0.09706305 -0.00135956  0.06978793]": "### 4 Typology of triangular at\n\nThere is a countably infinite way of cutting and recutting categories and subcategories of spectatorial activities",
    "[-0.0391617   0.13608326 -0.03694765 -0.02237968 -0.01919371 -0.0013588\n  0.06943546 -0.01690005  0.0093653   0.01919487]": "In one of the vertices, the art",
    "[-0.00836396  0.02556161 -0.0181757  -0.04965082 -0.02309541  0.00592919\n  0.00399176  0.08194997  0.00489013  0.0173548 ]": "On the base formed by a line drawn from the two remaining vertices, the audience",
    "[ 0.10725578  0.00196573  0.11116434  0.03031617  0.10124672  0.02956831\n  0.13424806 -0.05108931 -0.02603516 -0.02642718]": "This \n\n### 5 What am I to do when I get there?\n\nWhy do we view certain events as boring? Why are the plastic arts appreciated as tourism so popular, and art appreciation as a regular hobby so fringe? Why do people like to see Van Gogh with other people, an",
    "[ 0.05143059  0.04288391  0.07303558 -0.01243068  0.03165906  0.0204041\n  0.04201118 -0.0262257  -0.05563699  0.01219781]": "### 7 Why triangular art is primed to become infrastructural\n\nWhy is it that the plastic arts thrive in the context of tourism, but languish as a private hobby? How come that people from all over the world flock to the Louvre, the Met Museum, the Prado, th",
    "[ 0.02002572  0.07713469  0.02527213 -0.02363247 -0.01674944  0.01084491\n  0.05090154 -0.03302814 -0.0784521  -0.01220162]": "The touristic appreciation of the plastic arts \n\nThe last line of defense of triangular art is its quality as a live thing",
    "[ 0.07533738 -0.03678996 -0.00405601 -0.02126173  0.01895183  0.06164217\n -0.05368542  0.00437228  0.05091272 -0.00074609]": "To see that it isn't the case the people necessarily prefer live presentations, just compare the theatre to movies",
    "[ 0.11166915 -0.07251985 -0.04655567 -0.04563709  0.05004063  0.09666168\n  0.00369024 -0.00212309  0.02840529  0.05372449]": "Compare attendance in live cinema theaters and spectatorship over streaming services",
    "[ 0.04620999  0.01210728 -0.01098336  0.00219416 -0.04270817 -0.07787226\n  0.09163597  0.08889058  0.02416248  0.03860484]": "At a time, people would go to the \n\n### 8 A arrog\u00e2ncia da capital\n\nO best-seller, o espet\u00e1culo esgotado e a capital de todos os pa\u00edses t\u00eam mais em comum do que se costuma imaginar",
    "[ 0.08157973  0.02117165 -0.03023219  0.01056435 -0.04966748 -0.11200516\n  0.05738993  0.01401264 -0.03341486  0.03694434]": "Falo aqui de capital n\u00e3o no sentido c\u00edvico-pol\u00edtico, pois a capital descrita na Constitui\u00e7\u00e3o nem sempre \u00e9 aquela da imagina\u00e7\u00e3o",
    "[-0.01401446  0.04003985 -0.0759851   0.00826697 -0.0957891  -0.02679026\n -0.03728075  0.01697835  0.01393875  0.05546253]": "Apenas esta \u00faltima tem o poder triangular",
    "[ 0.04217391 -0.004646   -0.01921935  0.04586311  0.05404756  0.06392846\n  0.11050031  0.0297731  -0.0628181  -0.04368703]": "### 9 Benjamin sobre  Baudelaire\n\n\"In the flaneur, the intelligentsia sets foot in the marketplace, ostensibly to look around, but in truth to find a buyer",
    "[ 0.0236387  -0.03442486  0.00587527  0.02035091  0.01430835 -0.03644616\n  0.05336144  0.01435955 -0.03780343 -0.03831362]": "In this intermediate stage, in which it still has patrons but is already beginning to familiarize itself with the market, it appears as the boh\u00e8me",
    "[ 0.10617261  0.07418871  0.01462543 -0.07147168  0.07202782 -0.01908231\n  0.0368938   0.01880447 -0.04938886 -0.04163082]": "\" (Paris, Capital of the XIXth Century, V",
    "[ 0.06858274 -0.00653378 -0.01831179 -0.03602483  0.04512329  0.10650998\n  0.09561536 -0.01213018 -0.04910872 -0.127302  ]": "Baudelaire, or the Streets of Paris)\n\nThis observation about Baudelaire perfectly captures the difference between the artist under the patronage system and that of the artist as a producer in search of consumers",
    "[ 0.00449643  0.0646559  -0.09858081 -0.00911939 -0.05292179 -0.00107476\n  0.09232924 -0.05298907 -0.01727252 -0.03301905]": "The nobleman who allocates a certain sum of money to an artist or a group of artist cannot be seen as a consumer",
    "[ 0.10527218  0.05967724  0.0551822  -0.04657591 -0.03591639  0.01520058\n  0.15764077 -0.04381305 -0.01222656 -0.02746081]": "He is not buying art, he is, in fact, buying the aura of the arts and immersing itself in it",
    "[ 0.06974059  0.04033031 -0.04557727 -0.03718566 -0.03894398 -0.01219954\n  0.05269842 -0.05353968  0.03235552 -0.07472222]": "The bourgeouis who buys a painting or a sculpture is engaged in exchange, but the patron-artist relation has nothing of a market exchange",
    "[ 0.04825775  0.06182828 -0.01514091  0.00753508 -0.08519117 -0.03704002\n  0.02184732 -0.01560899  0.06482551 -0.0715021 ]": "Evidently, there is a difference between having the Church as one's patron (Michelangelo) or a nascent bourgeouisie (Rembrandt)",
    "[ 0.00524121  0.08062509  0.01452155  0.04691873  0.13669269  0.04072583\n  0.00642822 -0.01857751 -0.01226822 -0.07776859]": "But what is interesting to notice here is the difference effected when the artist had to step into the market instead of being protected in the palace",
    "[ 0.07857989  0.00846404  0.00715664 -0.01906302 -0.08777234  0.09289502\n  0.05849345  0.02460412 -0.02883822 -0.05284889]": "Due to the transformational nature of his time period, Baudelaire had to walk the fine line between pleasing a potential patron and finding a wide readership",
    "[-1.1136907e-04  3.2092191e-02  3.6541216e-02 -5.2363411e-02\n  7.8565411e-02  1.3829561e-01  1.0193221e-01 -6.3623615e-02\n  7.6801158e-02  3.2928415e-02]": "### 9 Why spectator art must hail the new as its God, yet pay the tithe to the repetitive\n\nHere we have one of the most interesting conflicts to be found in the contemporary spectatorship-dependent artist: that of marketing the novel while producing the re",
    "[-0.0014177  -0.00733904  0.05728452  0.0083231  -0.01786797  0.01267947\n  0.002344    0.00575706 -0.04418772 -0.0475933 ]": "\"Find your own style\" is a common advice",
    "[ 0.00126322  0.05813023 -0.01255684  0.01356625  0.04360427  0.10900319\n  0.08148029  0.05104901 -0.01770041 -0.02348433]": "Yet if one veers too much from what the market is buying, doom awaits at the end of \n\n### 10 A secret that makes you hateful to everyone else\n\nIn his Eugene Aram, Bulwer-Lytton orchestrated his description of big-city dwellers with a reference to [Goethe]'",
    "[-0.08639202 -0.01475727 -0.0065098  -0.03606337 -0.01419602 -0.06721416\n  0.08544943 -0.01516007 -0.01553678  0.08427444]": "(p",
    "[ 0.07760991  0.06021351 -0.00206701  0.02740305  0.05356491  0.14626108\n  0.0301984   0.01796077 -0.08847677  0.00513814]": "68, Benjamin Baudelaire Literary)\n\n### 11 \n\n\"Since the days of Louis Philippe, the bourgeoisie has endeavored to compensate itself for the fact that private life leaves no traces in the big city",
    "[-0.00269568  0.17405178  0.00576373  0.01220747  0.02037917 -0.02288941\n  0.04023573 -0.04742613 -0.03351989 -0.01229   ]": "It seeks such \"Compensation within its four walls-as if it were striving, as a matter of honor, to prevent the traces, if not of its days on earth then at least of its possessions and requisites of daily life, from disappearing forever",
    "[-0.06031001  0.02556841  0.01267209 -0.01329981  0.00683219 -0.04316871\n  0.12984867 -0.03098583  0.00839704  0.03309825]": "\" (p",
    "[-0.01773328  0.02929543  0.00350829  0.05558536  0.00238947  0.14291038\n  0.06261181  0.035142   -0.10478141 -0.05462509]": "77)\n\n### 12\n\nAs early as 1836, Balzac ~rote in Modeste Mignon: \"Poor women of France! You would probably like to remain un- known, so that you can carry on your little romances",
    "[-0.00429997  0.00621934 -0.0229805   0.02692771 -0.05145782  0.07598689\n -0.02279929 -0.07239127 -0.00333311 -0.0029231 ]": "But how can you manage this in a civilization which registers the departures and arrivals of coaches in public places, counts letters and stamps them when they are posted and again when they are delivered, assigns numbers to houses, and will soon have the ",
    "[ 0.05594348  0.0393499   0.01419556 -0.0302399  -0.04462126  0.02900738\n -0.14976364  0.02342197 -0.1401139   0.00616641]": "The numbering of houses in \u00b7 the big cities may be used to d9cument the progressive standardiza- tion",
    "[ 0.03336804  0.05079646  0.04792246 -0.00836299  0.06242563  0.11354168\n  0.05869552 -0.02069279 -0.08522356 -0.00447014]": "78)\n\n### 13 The old opposition between mass and private art is not as useful as it used to be\n\nThe distinction between high and low art should be discarded, not because of its elitist bent, but due to its uselesness in the contemporary world",
    "[-0.01249884 -0.00522321  0.08977549 -0.06304269  0.0789682   0.05450875\n -0.00509867  0.04377268 -0.01921482  0.01132269]": "### 14\n\n\n### 15\n\"Dandies are becoming rarer and rarer in our country, whereas amongst our neighbours in England they social system and the constitution (I mean the real constitution: the constitution which expresses itself through behavior) will for a long",
    "[-0.04636277  0.0355085   0.0012526  -0.0521231   0.0532544   0.09579113\n  0.08996404  0.0377415  -0.05163253 -0.07244872]": "\" (Charles Baudelaire in The Painter of the XIXth Century)\n\n### 16 Mark Fisher\nhttps://www",
    "[-0.04910229 -0.10689563 -0.05166606 -0.0539257   0.03961264 -0.00622751\n  0.00167652  0.0394568   0.03573374  0.00774471]": "youtube",
    "[-0.01706496  0.11311574  0.04215443  0.00603983  0.05932381 -0.01208726\n -0.04364258  0.06224538 -0.07919692  0.06027815]": "com/watch?v=8Bk0kkRPmjE&t=1830s 17:30\nHow to resist technology without becoming reactionaries?\n\n\nIt's kitsch, it's just kitsch",
    "[ 1.2729389e-02 -1.6444061e-02 -6.8852540e-05 -4.8047956e-02\n -2.2251263e-02  1.7474450e-02 -3.0979199e-02 -9.6233329e-03\n  1.1474114e-02  7.8307398e-02]": "# An essay on a much needed distinction\n\n### Generative AI\n\n### Triangular and circular activities\n\n### Why triangular/circular and not passive/active, high/low, commercial/artistic\n\n### Preserving",
    "[-0.04379533  0.17905787 -0.06960113 -0.05951212  0.0058298   0.02089763\n  0.06946333  0.07277428  0.03610634  0.0053994 ]": "What would you have done had you received a death sentence from your doctor when you were a teenager? This is the question Ippolit Terentyev, a secondary character from Dostoevsky's novel 'The Idiot' has to struggle with",
    "[-0.02087216 -0.00647868  0.03360887 -0.07064048  0.04531458  0.03849429\n  0.03117576  0.08241496  0.03419875 -0.00917139]": "I would like to make the case that the opposition between Prince Myshkin and Ippolit is \n\nTo be honest, although I really wanted to like Myshkin in the beginning (specially when Dostoevsky himself hinted to his ) I simply cannot \n\nIppolit's central arc con",
    "[ 0.01047813  0.12412909 -0.08870552 -0.07764744  0.07769652  0.08126018\n -0.01464219  0.11649833 -0.03711602  0.02165214]": "The young boy received a death sentence very early in his life: he contracted tuberculosis and does not have that long to leave",
    "[ 0.10376906  0.0493792   0.0197468  -0.01215435 -0.0889673   0.00789609\n -0.01022651 -0.06624917 -0.08083583 -0.04565121]": "He will not get to know adulthood, or raise a family, or do well at a job, or any of the things he sees those around him perform",
    "[-0.02009909  0.0349854   0.06403403 -0.04338741  0.0345237   0.00134279\n  0.09379458  0.06252212 -0.00381962 -0.01948608]": "Some people *really* dislike Ippolit",
    "[-0.01598272 -0.02177982 -0.04101954  0.04547746  0.01599227  0.0247077\n  0.00608452  0.06198615  0.04334439  0.05937602]": "If you check out some reviews on Goodreads and other platforms, \n\nIn the end, 'The Idiot' is a disappointing novel but I do not regret reading it",
    "[ 0.00344739  0.01569974  0.00673465  0.03730657  0.10812347 -0.03907915\n  0.07988063  0.00152812  0.01210424  0.00281016]": "In fact, it sort of ignited",
    "[-0.0177911  -0.07062791  0.02008046  0.00892349  0.08190022  0.00225383\n -0.05952664 -0.14216477 -0.03850418  0.03027462]": "The expedition has gone awry, the party has been disbanded and conflicts have risen inside the group",
    "[ 0.00979385 -0.00544823 -0.00722936  0.02777636 -0.03472056 -0.02538685\n  0.05080236 -0.09482076 -0.00494253 -0.01030674]": "In the midst of all of this, they choose one man from their party to act as the King of Spain",
    "[ 0.01346667  0.04698523 -0.00629513  0.01057482  0.09839141 -0.05463904\n -0.07189374 -0.04609945  0.05249915  0.06278326]": "The law arrives in horses, but the world moves at the speed o",
    "[ 0.03398204  0.01469162  0.04365105 -0.01713921  0.05476372  0.05109926\n  0.15968609  0.09404689  0.02925777  0.04502321]": "## Introduction\nWhen we are confronted with the topic \"Wittgenstein and AI\"\n\nI propose two initial questions:\n\n## Wittgensteinian analysis of AI\nThe first question is this: can we use Wittgenstein to comprehend AI?\n\nThis approach is the one most suitable t",
    "[-0.0269061  -0.09142461  0.01237299 -0.01170473  0.02037972 -0.02455492\n  0.02495977  0.02516069 -0.04291776  0.01397617]": "Luckily, I believe many of the debates currently going on in the AI community, specially those concerning the creation of \"Strong AI\" or \"Artificial General Intelligence\" have been, in some sense, anticipated by his later thought",
    "[ 0.02018202  0.02987259 -0.02051241 -0.05613935  0.05969716 -0.04532111\n  0.04325503 -0.06345218 -0.05547662 -0.04308948]": "Take the task of binary classification of images",
    "[ 0.06998184  0.04057616 -0.02044277 -0.0241201   0.07539321 -0.10444029\n  0.01882558 -0.07183265  0.03582377 -0.07131528]": "We want to know, given a dataset, which images are pictures of a certain object and which are not",
    "[ 0.01346659 -0.04511688 -0.01399948  0.03832154  0.09892196 -0.04261755\n -0.04780423 -0.0097957   0.06882177 -0.01087414]": "This can be as simple as having our model say that something is or isn't a picture of a cow, for example",
    "[ 0.00345096 -0.04313092  0.04965351  0.08072412  0.12311686 -0.00720101\n  0.01233321 -0.04945492  0.02540944 -0.02181843]": "In supervised learning, we would train our model by proceeding like this: First we select a large sample of images, some containing cows and some containing anything else",
    "[ 0.02843139  0.02033907 -0.0219351   0.0079255   0.04677814 -0.05082064\n  0.02751883 -0.04481551  0.1146758  -0.05993142]": "Then, we get humans to label such images",
    "[-0.03161589 -0.01393847 -0.06366558  0.00013097 -0.0015953   0.0238924\n  0.05536493 -0.02324521  0.05668806 -0.09025957]": "The labels can say either \"cow\" or \"not cow\"",
    "[ 1.2359045e-02 -6.4070948e-05 -5.0500192e-02 -5.4888714e-02\n  2.0411760e-02 -6.8709247e-02 -8.8682547e-02 -4.3551836e-02\n -2.5500623e-03 -8.6739384e-02]": "The next step is to transform these images, properly labeled, into a numerical representation (that's what computer scientists call \"encoding\" the data)",
    "[-0.06936805 -0.04067567 -0.01702199  0.02739666  0.10414714  0.02636222\n  0.03185118 -0.011842    0.0403792  -0.07099085]": "By feeding the data into our neural network, the system would guess whether a sample depicts a cow or not and compare its prediction with the human crafted labeled",
    "[-0.08479077  0.01967219  0.04108271  0.06529314  0.02541348 -0.12755743\n  0.01036645 -0.02219385  0.04555356 -0.04968569]": "If it is wrong, it modifies its weights and tries again for a certain amount of time until it stops and is tested against our test data: images that it has never seen before",
    "[-0.06671914  0.02949223 -0.0288513  -0.00772328 -0.05327812 -0.05249141\n  0.0195857  -0.04848078  0.08784001  0.00147228]": "Don't we have here a perfect reproduction of the Augustinian picture of language? An image is shown, a name is attached to a picture, and by trial and error our system learns a \"name that signifies a thing\"",
    "[ 0.02887098 -0.08933801  0.0408522   0.03534427  0.04139787 -0.00397732\n  0.06242795  0.01428353  0.09711123  0.00474528]": "But how do we make sure that what the model is learning is indeed the cow part? Wittgenstein talked about the experience of pointing to an object, to its shape, to its color, to its \"number\", etc",
    "[ 0.04221915 -0.05681304  0.01220699  0.00519551  0.1309251  -0.05746472\n -0.05331732 -0.02469905  0.10045701 -0.02179011]": "How do we know that our model is indeed looking at the figure of a cow?\n\nIn fact, if in our dataset all the cows were depicted in their familiar environment, that is, green pastures, it is *entirely possible* that the model would say that an image does not",
    "[-0.01265425  0.02015747  0.00736667 -0.0421227   0.02652228 -0.08308971\n -0.07700358  0.02236308  0.00556202 -0.00100213]": "The problem can be extended to pictures of cows from weird angles, or if the animal has been rotated, etc",
    "[ 0.03109134 -0.02197573 -0.01739712 -0.10565779 -0.04132051 -0.00430029\n  0.01971417  0.05935217  0.08492205  0.0384708 ]": "There are countless problems like these with contemporary systems, and many of them could be the object of intense thought in terms of language games, rule following, meaning as use, etc",
    "[-0.03127022 -0.02182919 -0.04550441 -0.05957632  0.0219845  -0.00669116\n -0.07720251  0.02081983  0.05330189  0.04008024]": "It is astonishing just how many passages of the Philosophical Investigations mirror issues with contemporary systems",
    "[ 0.00353943 -0.05580421 -0.03921649 -0.01657667 -0.02362373  0.04579623\n  0.0512684   0.060531   -0.01235164 -0.02958282]": "Think of his analysis of reading in sections 156 and 157 of the PI, where he describes people who have been brought up Could we apply this line of reasoning to large language models such as GPT-3? Could we say of them that they are reading? Does it matter?",
    "[-0.01712196 -0.06092105  0.01055266  0.00108676  0.04205234  0.02587035\n  0.08813376  0.04392203  0.04681519  0.05475962]": "## Modeling AI through Wittgenstein\nAnd this second question is: can we use Wittgenstein to *build* AI?\n\nAlthough less likely to be taken up directly by philosophers, this question is nonetheless a crucial one",
    "[ 0.08206391  0.02582567  0.01338158  0.00170373  0.07178395 -0.03559174\n  0.09043428  0.04624873  0.01315645  0.01326677]": "As it is known, Wittgenstein pushed his students to abandon philosophy and take up a practical vocation outside academia",
    "[ 0.06967078  0.08128957  0.04902191  0.02654488  0.01447716 -0.01227215\n  0.01262872 -0.01078657 -0.02433895 -0.00083757]": "He himself tried to let go off his academic commitments altogether in numerous occasions",
    "[ 0.05378464  0.08474985 -0.03380756 -0.01426408 -0.05598786  0.00575025\n  0.06995267 -0.01458577  0.00247939  0.02164192]": "Think of his foray into architecture and into teaching kids",
    "[ 0.00980245 -0.00714772 -0.0170521   0.04840973  0.01413361  0.04603793\n  0.00692672  0.03601668 -0.13139148  0.01352171]": "Think of his plans of training to become a medical doctor or to leave Cambridge and join a collective farm in the Soviet Union as a manual laborer",
    "[ 0.03570989  0.00891371 -0.0329423  -0.02199946  0.02064632 -0.03126489\n  0.02188909  0.03367961 -0.0565144  -0.00614796]": "Even though he had a distaste for thinking of philosophy as something akin to science, or that could aid science somehow, using his thought as inspiration for building interesting machine learning models could prove a fruitful, practical endeavor",
    "[ 0.00322246 -0.05073652  0.03559686 -0.0088153   0.03876384  0.04185199\n  0.11865386  0.07827765  0.03506269  0.04307101]": "Surprisingly, I have been able to find papers in AI research that take Wittgenstein's ideas about language seriously, and try to build models to learn \"language games\", or to use \"language games\" as occassions for learning how to reason about color, quanti",
    "[-0.07988707 -0.09736538 -0.04587494 -0.0176379   0.03974991 -0.07020786\n -0.10020018  0.04611086 -0.03695587  0.02693232]": "I'm afraid I don't have time to go in detail about these systems right now, but they do exist and are being pushed by some of the most advanced labs in AI research in the world",
    "[ 0.04900742 -0.04942269 -0.02753793 -0.02113044  0.00871249 -0.01413672\n  0.08694354  0.07036854  0.09116403 -0.01397094]": "In fact, when you think about it, beyond comparing language to a toolbox, Wittgenstein also assembled an interesting philosophical toolbox himself",
    "[ 0.0405526   0.01402613  0.00266921 -0.06556884 -0.08225741  0.03843692\n  0.09573325  0.02102715  0.02155938  0.03969026]": "The notions of language games, meaning-as-use, forms of life, grammar, hinge propositions, etc",
    "[ 0.01853819  0.03988343 -0.0948768  -0.05661587  0.02620607 -0.04961872\n  0.07481471 -0.030603    0.03675979  0.02027708]": ", can all be thought of a blueprint for how human-like  \n\nI don't think philosophers should turn up their noses against the appropriation of philosophical ideas by non-philosophers, even though they sometimes do not make exegetical sense or have not been t",
    "[ 0.02956116  0.02143222 -0.0008868  -0.02218871  0.02672462  0.01358174\n  0.10726285  0.0569264   0.08278114  0.07090069]": "## The third option\n\nIn his 'Lecture on Ethics', Wittgenstein said that a definitive book on ethics would, with an explosion, destroy all the other books in the world",
    "[-0.02100292  0.04855093 -0.05278211 -0.04664144 -0.00755064  0.01845197\n -0.09892767 -0.04117939  0.02980476  0.11255191]": "If we were capable of treating ethics as a natural science or as a quasi-mathematical system in the manner of Spinoza, the endpoint of such an investigation would mean that every other word about the subject ceases to have importance",
    "[-0.01892673 -0.0345543   0.00569104 -0.03438723  0.01898672 -0.0207405\n  0.04370663  0.06841685  0.02314633  0.03349948]": "With this in mind, we should ask: what can AI do to Wittgensteinian philosophy?\n\nLet's say a new model is invented that can perform very well in certain well-designed benchmarks",
    "[ 0.01580228 -0.05919797  0.08553527  0.02758657  0.03454303  0.04511325\n -0.08053155  0.04792733  0.07458658  0.03485157]": "Let's say further, that in order to achieve these phenomenal levels of success, the model had to be enriched with priors regarding space, time and causality, as its innate, most basic components",
    "[-0.02712047  0.00586513  0.03375293 -0.01762683  0.06558624  0.04467958\n  0.06922043  0.09536844  0.03732324  0.04637053]": "Would we be willing to accept this as evidence that Kant was right and Wittgenstein was wrong? Would we accept *anything* coming from AI research as evidence against the idea that meaning is use? Is it right to worry that \n\nCould properly built AI destroy ",
    "[-0.02921029 -0.00813602  0.03420505  0.04029561  0.07426187  0.05922511\n  0.0818095   0.03713739  0.03291481 -0.03143711]": "l",
    "[ 0.01912679 -0.00890972 -0.11796872 -0.05217984  0.01807565 -0.0242199\n  0.03194245 -0.00238728  0.04538686  0.05458657]": "The power to create systems that exhibit human-like performance and/or behavior, even in narrow tasks, should give us pause to think about all of our assumptions when we do philosophy",
    "[ 0.03491527  0.03440016  0.0100761   0.05297166  0.02586204 -0.04193782\n -0.05495136  0.08237157  0.06948907  0.04743981]": "I don't want to be pessimistic nor come to hasty conclusions",
    "[-0.04069803  0.04829362  0.02121465  0.01411339  0.07989337  0.01014703\n  0.00049502  0.01643711 -0.13719627 -0.03924153]": "[[Regularization]] refers to a set of techniques created to mitigate [[overfitting]] in models",
    "[-0.05187293  0.02442116  0.05537496  0.04416878  0.01084504  0.05981835\n  0.04040528 -0.06466022 -0.04008425 -0.04426488]": "Regularization is applied on a model's *weights*",
    "[-0.0071154  -0.03031205  0.02055093 -0.05070489  0.01295346  0.00978646\n -0.01155815 -0.00251882 -0.12336127 -0.04039402]": "[[L1 regularization]] and [[L2 regularization]] are the most common techniques, widely used and taught",
    "[-0.02726396  0.06417073  0.00159431  0.02384083  0.06670204  0.12722504\n -0.00592645  0.01651764  0.0577828   0.03877577]": "The basic idea is to add a *penalty term* to the loss, penalizing certain behavior of the model",
    "[ 0.01296323  0.00410986  0.04740529  0.02147313  0.12167994  0.05648004\n  0.08203115 -0.04671445 -0.07491375 -0.02507236]": "### [[L1 Regularization]] ([[Lasso Regularization]])",
    "[-0.00665865 -0.008745    0.06068163  0.01032769  0.11678502  0.08907755\n -0.01354734  0.01720315 -0.07865929 -0.02816499]": "In [[L1 regularization]] we change the loss $\\mathcal{L}$ to sum the absolute value of the magnitude of the coefficients to it:\n$$\n\\mathcal{L}(x) \\rightarrow \\mathcal{L}(x)",
    "[-0.01150145  0.02108842 -0.04567214  0.02265163 -0.00010789 -0.00981402\n  0.04986926 -0.06774056 -0.00314921 -0.04307044]": "\\lambda \\sum_{i=1}^{n} |w_i|\n$$",
    "[ 0.02668607  0.02428832 -0.0841626  -0.00925731 -0.06777593  0.05167218\n  0.00958072  0.00314849 -0.02875879  0.03188693]": "Where $\\lambda$ is a hyperparameter ranging from 0 to 1, controlling the strength of the penalty",
    "[-0.03654454 -0.0820969   0.10753259  0.08704352  0.09992927  0.01882984\n -0.0180181  -0.02696741 -0.05655191 -0.06418062]": "L1 regularization encourages *sparsity* in the model's weights, thought to be a form of *feature selection*",
    "[-0.04020549 -0.02100519  0.02057874 -0.06251707  0.12624915 -0.00152943\n -0.05446345  0.04087758 -0.08610851 -0.00197701]": "**Disadvantage of L1 regularization**: The absolute value function used in L1 regularization is not differentiable at zero",
    "[-0.00090072  0.03771555  0.0212627   0.00291226  0.06685606  0.00437162\n  0.00427982 -0.00584777 -0.07837566 -0.04044344]": "### [[L2 Regularization]] ([[Ridge Regularization]], [[Weight Decay]])",
    "[-0.01098472 -0.00305803  0.07508272  0.02853268  0.05967023  0.05689839\n -0.02123186 -0.00209538 -0.0594622  -0.04543616]": "In [[L2 regularization]] we change the loss $\\mathcal{L}$ to incorporate the squared magnitude of the weights:\n$$\n\\mathcal{L} \\rightarrow \\mathcal{L}",
    "[-0.02563931  0.02775635 -0.01765068  0.06413884 -0.0682714  -0.02267212\n  0.01271096 -0.06757609  0.03642534 -0.04253675]": "\\lambda \\sum_{i=1}^{n} w_{i}^{2}\n$$",
    "[-0.03701812 -0.03398015  0.04371827  0.00823889  0.10763782  0.01329405\n -0.02982576  0.02115504 -0.10932108 -0.01883088]": "**Disadvantage of L2 regularization**: L2 regularization is **NOT** robust to outliers",
    "[-0.02477327 -0.01315119  0.05358267  0.07344951 -0.00192365 -0.03616233\n -0.07562638 -0.00089783 -0.04111632 -0.0484522 ]": "The squared terms will blow up the differences in the error of the outliers, and the regularization might make the model fight the size of its weights and not the outliers themselves",
    "[ 0.02262838  0.06634212  0.02569254 -0.00226225  0.06654825  0.03060557\n  0.14362156 -0.00654179  0.05726851 -0.10166568]": "### [[Dropout]]",
    "[-0.00852961 -0.00808478  0.04442998  0.04785219  0.03665429  0.04935323\n  0.0498933   0.03646956  0.07340803 -0.08285272]": "The idea behind dropout is to ignore the output of a subset of the neurons in a layer, typically a number around 20 and 50% (with 20 being more common)",
    "[-0.03254844 -0.01458306  0.0607342  -0.00817674 -0.01452004  0.01710974\n  0.04605163  0.00347938  0.04209105 -0.02791597]": "The dropout method is applied independently to each neuron with a probability $p$, given as a hyperparameter",
    "[-0.04005826  0.0055061  -0.02859513  0.04464954  0.01149316  0.01710317\n  0.0795093  -0.04388257  0.02371698 -0.09799536]": "A concept connected to dropout is that of [[Drop Connect]]",
    "[-0.06266461 -0.06137947  0.09300797  0.06493751  0.01006794  0.00230052\n  0.07462998 -0.09164807  0.09608188 -0.00145883]": "Here, instead of negating the output of a random neuron, only certain connections between neurons are dropped",
    "[ 0.01270854  0.04748999  0.01095314  0.03579462  0.08270736  0.05830986\n  0.04290889  0.00369294  0.01402023 -0.05666972]": "**Important**: for obvious reasons, dropout is only done during training, not during inference",
    "[ 0.0398142   0.08714397  0.04579457 -0.00133025  0.09715313  0.09336685\n  0.06636719  0.0470411   0.0307532  -0.02176292]": "### [[Early stopping]]",
    "[-0.03389005  0.02617741  0.07358423  0.05388826  0.136974    0.05750284\n -0.04525121 -0.01205637 -0.02276059 -0.00877188]": "Early stopping can be employed to prevent a model from being trained to the point of overfitting",
    "[-0.03119431  0.02875873  0.02132768  0.03410103  0.09585514  0.09264932\n -0.02922145  0.03353026  0.03239784 -0.00221337]": "The point at which the training loss keeps going down but test lost loss just starts going up is a good candidate for early stopping",
    "[ 0.0163478   0.08643835  0.04099878 -0.04225282  0.0885561   0.04729492\n  0.07341956  0.01264412  0.00655027 -0.02261387]": "![[Early Stopping",
    "[-0.00360448  0.0392359   0.0233902  -0.00388128 -0.01369659 -0.00232555\n  0.11165387  0.01111939 -0.09185404 -0.02932328]": "png]]\n\n### Batch Normalization",
    "[-0.0088537  -0.0453495  -0.01740544  0.03225812 -0.03502215  0.04416612\n -0.03811543 -0.01720726 -0.13052653  0.00508454]": "Batchnorm is mainly a form of *normalization*, with *regularization effects*",
    "[ 0.03725412 -0.03663386 -0.10842316  0.04681524 -0.06896754  0.03262132\n -0.01098963  0.02143397 -0.17147379 -0.03372745]": "See the page [[Normalization]] for more details on batchnorm",
    "[-0.01378549  0.09577942  0.02636499  0.00048782  0.03097019  0.05788829\n  0.12084709  0.04548015  0.0855906  -0.0504497 ]": "# [[Loss Function]]",
    "[-0.02053733  0.00578237 -0.0475612  -0.06522429  0.0126781   0.05171368\n  0.01109062  0.02300157  0.0664982   0.02492575]": "Different loss functions are to be used for different learning objectives",
    "[-0.01652803  0.04472997 -0.06401183 -0.01125132  0.09464034  0.02694844\n  0.05012677 -0.05950945 -0.00802311  0.04099293]": "We divide objectives into [[regression]] and [[classification]]",
    "[ 2.5906833e-05 -4.1662656e-02 -4.0004980e-02  3.2710873e-02\n  4.8399740e-04  2.2117184e-02 -4.8195314e-02 -2.8457481e-02\n -4.2682495e-02  3.1546291e-02]": "Regression problems consist in predicting a *continuous* value, while classification problems consist in learning to predict from a set of *categorical variables*",
    "[-0.00634513  0.03658972  0.01519089  0.10257716  0.06434347  0.06107672\n  0.04497262 -0.00741584  0.04075807 -0.003303  ]": "# Regression [Regression]",
    "[-0.03131709  0.03810717 -0.02139815  0.0414594   0.01591692  0.08806752\n  0.00200961  0.0318514   0.00882067  0.01893763]": "A regression problem consists in predicting a single continuous variable such as the price of a house, the estimated selling price of a piece of art in an auction, a player's ELO in chess, and many other similar tasks",
    "[-0.01093004  0.04903173  0.03570736  0.07459685  0.03638961  0.04741754\n  0.0365223   0.06459258 -0.01857935  0.00739835]": "### Loss Functions for Regression:\n\n**[[MSE]] - Mean Squared Error (MSE)**",
    "[ 1.3175984e-02  4.0140565e-02 -3.6659832e-05 -1.4152467e-02\n -2.1801872e-02  5.4837078e-02 -3.1931363e-02  1.2574216e-02\n -2.5102800e-02  4.2304032e-02]": "Mean-squared error is the most basic objective function for regression problems",
    "[-0.03583069  0.03605622  0.03727233 -0.01468494 -0.02059808  0.0233444\n -0.05729995  0.00075915  0.05178529 -0.0057901 ]": "It consists in the following equation:\n\n$$\n\\frac{1}{n}\\sum(Y_i - Y^{pred}_i)^2\n$$",
    "[ 0.04255059 -0.04921617 -0.00913752 -0.00108365  0.00889174  0.0536738\n -0.00854324  0.0241526   0.00999326 -0.03714219]": "Where n = the number of examples in the dataset",
    "[ 0.01842444 -0.03664692  0.02451449  0.00186421 -0.02876626 -0.04451238\n -0.10580347  0.07571615  0.00970634  0.03592419]": "Its offshot, [[RMSE]] - Root Mean Squared Error (RMSE), works by taking the square root of the expression:\n$$\n\\sqrt(\\frac{1}{n}\\sum(Y_i - Y^{pred}_i)^2)\n$$",
    "[ 0.03297995 -0.00226649  0.03768542 -0.01763355  0.01264675 -0.04448028\n -0.04078551  0.04044209 -0.02481279  0.01477714]": "It is also possible to work with [[MAE]] **Mean Absolute Error** (MAE), which simply takes a mean of the absolute error:\n$$\n\\frac{1}{n}\\sum(Y_i - Y^{pred}_i)\n$$",
    "[-0.0582376  -0.05152947  0.04857077  0.0425193  -0.01272798  0.0096416\n  0.0046454   0.1467407   0.0441312   0.02929491]": "**MSE** is usually preferred because it penalizes outliers, which contributes to a smoother loss curve",
    "[-0.02706689  0.02723688 -0.01630241  0.01408787  0.07183006  0.05198206\n  0.12249891 -0.04646626 -0.06069196 -0.01833063]": "#  Classification [Classification]",
    "[-0.04488498  0.00941768  0.02016005 -0.05745403  0.03057124  0.03165132\n  0.10371592 -0.05895153 -0.04768115 -0.00858786]": "A classification problem consists of deciding to which class a datapoint belongs to",
    "[-0.0638883   0.03790284 -0.10436184  0.00019331  0.04589133  0.03025731\n  0.08130808 -0.04930599  0.00959057  0.0233582 ]": "Classification can be used to predict whether a review is positive or negative, which objects a picture shows, fraud detection, and many others",
    "[ 0.04442682 -0.03758177 -0.04316138  0.02268461 -0.00387918  0.02131584\n -0.0258355  -0.08120658  0.00220009  0.00853789]": "In classification we work with *categorical variables*, instead of continuous ones like in regression",
    "[-0.00625871 -0.01890373  0.0112985  -0.05557377  0.05101021  0.03427947\n  0.02826759 -0.04637986 -0.03306321 -0.01542427]": "# Loss Functions for Classification\n\nFor [[binary classification]] problems, we use Binary CrossEntropy, and in the general case the [[Cross Entropy Loss]]",
    "[ 0.04202909  0.03870782  0.00799835 -0.04109403 -0.03110342 -0.01869584\n  0.13499056  0.00600783  0.0555098  -0.08918254]": "# [[Perplexity]]",
    "[-0.02819715 -0.06990279 -0.05897155 -0.03999596 -0.06046253  0.01761218\n  0.10244787  0.07291116  0.11759093 -0.0187387 ]": "[[Perplexity]] is a metric used in the language modeling task (guessing the next word based on previous words according to the general formula $p(x_t \\",
    "[-0.05877214  0.02016113  0.01407448 -0.03565244  0.05774245 -0.04126004\n  0.07996935 -0.00372256  0.07041397 -0.00595072]": "|\\",
    "[-0.08260991  0.06236726  0.01897589 -0.0417113   0.01379134  0.01208614\n  0.09372137  0.01470774 -0.01744696 -0.0214377 ]": "x_{0}",
    "[-0.16086756  0.02793771  0.07906555 -0.03857701  0.05976962  0.01173504\n  0.0900481   0.02424965  0.03363995  0.00271335]": "x_{t-1}$)",
    "[-0.03143271 -0.08689851  0.0149654  -0.04402287 -0.01395788 -0.01288149\n  0.06014268  0.00769796  0.13200794  0.06217987]": "In a Markov Decision Process only the previous word is used, in what is called one-gram language modeling",
    "[-0.05982561 -0.04774382 -0.01438056 -0.05285558 -0.04631253  0.06290612\n  0.1243069   0.03065478  0.08334272 -0.02977311]": "The idea can be generalized to n-grams",
    "[-0.00541172  0.00453455 -0.04788528 -0.02329824 -0.00582539  0.07576492\n -0.04995645  0.0329272   0.01384545 -0.0395408 ]": "In general, for the language modeling task, these are the necessary steps in order to do any sort of evaluation:",
    "[-0.07381174  0.07235789  0.00777574  0.05200035  0.08343922  0.09636304\n -0.01346256  0.00999296  0.0636638  -0.01954882]": "1) Give the model a piece of unseen text",
    "[ 0.00072201 -0.07451279  0.00920798  0.00176133  0.01793766 -0.00515589\n  0.13011624 -0.0052772   0.0026737   0.03521566]": "2) Check its next token prediction (a probability density over a fixed vocabulary)",
    "[-0.00936011  0.03620831 -0.0148073  -0.08685414 -0.05790827  0.02396127\n  0.14655077  0.02097348  0.03347487 -0.05470497]": "3) Compute a metric",
    "[ 0.03622905  0.02718359 -0.05573247 -0.00762186  0.02090801 -0.03208637\n -0.00770601  0.03402606  0.0717952   0.00923611]": "It's actually not very intuitive how to do an evaluation",
    "[ 0.03039784  0.03246905 -0.06129467 -0.02775356 -0.00034598 -0.05312456\n  0.06112854 -0.03416919 -0.02733461 -0.03934921]": "A naive approach might be to use the [[accuracy]]",
    "[ 0.02037736 -0.05035217 -0.02514473  0.03189522  0.06231934 -0.01015679\n  0.01333614  0.03661805  0.03908423  0.00868289]": "For example, given the sentence  'Why did the chicken cross the _ ', we might say accuracy is 100% if the model guesses \"road\" and 0% otherwise",
    "[ 0.02254889 -0.05995391  0.04300331 -0.02070263 -0.08242542 -0.00502656\n  0.07807388 -0.08870113  0.0192899  -0.0126959 ]": "This naive idea does not fully work, because in the actual distribution of human languages there are many candidate words that could work just fine, such as \"street\"",
    "[ 0.06534611 -0.09548233  0.0569154   0.02573139 -0.06652825 -0.05375345\n  0.02042154  0.00174112  0.07661526 -0.09015012]": "Obviously, street and road are very similar words and we want to bake that intuition into our metric",
    "[ 0.04288697 -0.07333163 -0.03557393  0.00451089 -0.04450881  0.00534965\n  0.08200646  0.02656281  0.07608072  0.05717625]": "Moreover, the human language distribution is what is known as a [[Zipfian distribution]], where a few words constitute the most of vocabulary usage, while some words are much rarer",
    "[-0.04927103 -0.07219603  0.02165915  0.06917144  0.0106194   0.00490891\n  0.00046901 -0.03588236  0.10026686  0.04047655]": "This means that not all predictions are created equal: it's much easier to guess words like \"the\", \"he\", \"she\", etc",
    "[ 0.09407096  0.03314558  0.01265662 -0.00987723 -0.0395313  -0.07797293\n -0.01175933  0.0187178   0.01314443  0.0080102 ]": "than words like \"verisimilitude\"",
    "[ 0.02205549  0.04301031 -0.07069604 -0.01520855 -0.04981628 -0.06184763\n  0.03807464  0.00147617  0.04368892 -0.04678302]": "This is where ***Perplexity*** enters the picture",
    "[ 1.3182932e-02  2.9357839e-03 -3.1114712e-02  1.0413997e-02\n -4.0942058e-02  1.5670090e-04  1.3008828e-01 -4.1674070e-02\n  1.5687361e-01 -2.5329705e-02]": "Perplexity is related to the concept of [[entropy]] in [[information theory]]",
    "[ 0.03940409 -0.11976557 -0.02197701  0.00014106  0.00825425 -0.03287642\n -0.11378927  0.0246462   0.05644057 -0.05503137]": "Notice that the perplexity of two different models is only absolutely comparable if the two models use the exact same vocabulary",
    "[ 0.00809242  0.00364061  0.02219431  0.02248546 -0.05511041  0.0485447\n  0.06115916  0.04865757  0.0933575  -0.03628505]": "## The general formula of Perplexity",
    "[-0.00278002 -0.06477185 -0.05743163  0.03690132 -0.08236504 -0.02511747\n  0.08869433  0.06454296  0.09227093  0.06756888]": "From Jurafsky's book, *Perplexity* for $P$ a probablity distribution, $N$ a vocabulary, and $W$ a test set (a corpus of tokens, essentially), is defined as the inverse of the probability of $W$, normalized by the vocabulary size, mathematically:\n$$\nPPL(W) ",
    "[-0.10142348 -0.00285843  0.01627377  0.0481847   0.05279558 -0.07173613\n  0.14971271  0.03863768 -0.05253151 -0.04749765]": "w_1",
    "[-0.11822332  0.0133572   0.02726685  0.06936303 -0.00449152  0.01413154\n  0.02652012 -0.02387727 -0.0187046  -0.06693453]": "w_{n-1} \\}}}}\n$$",
    "[-0.04620349  0.0044177   0.01608788  0.04579331 -0.03617536 -0.04964212\n  0.08444843 -0.0017305   0.06870209 -0.00329998]": "We can expand the formulation using the chain rule to expand the probability $P$:\n$$\nPPL(W) = \\sqrt[|N|]{\\prod_{i=1}^{N} \\frac{1}{P(w_i \\",
    "[-0.09402557  0.04829555  0.01613691  0.0438137   0.01609559 -0.0317789\n  0.07987016 -0.03072535 -0.00433446 -0.02898306]": "w_{i-1})}}\n$$",
    "[ 0.02203434  0.00936808  0.00400622 -0.02341916 -0.08814198 -0.0358159\n -0.02412966  0.08890399  0.06176168 -0.04848148]": "The lower the *Perplexity*, the better",
    "[-0.05858891 -0.05779113  0.05930775  0.12240878  0.09782513 -0.0546344\n  0.00725676 -0.01952532  0.13346615 -0.0251954 ]": "Intuitively, the more \"surprised\" the model is to see $w_i$ after having seen $w_1",
    "[-0.09510608 -0.00745972  0.04767574  0.08579155  0.05868144 -0.08680739\n  0.02738697  0.01052224 -0.00800362  0.00882824]": "w_{i-1}$, the worse it has learned the distribution",
    "[ 0.03565694 -0.05774635 -0.04945268  0.07110514  0.04846181  0.0712767\n  0.018482   -0.02518944  0.02908217 -0.02571633]": "Observation: a Perplexity score of 1 means the model is fully deterministic, and a Perplexity score of $|N|$ means the model has not learned anything and is fully randiom",
    "[ 0.01425229 -0.00093015  0.04208564  0.06121888 -0.02661064  0.07257532\n  0.12502426 -0.01519411  0.12619391 -0.04930008]": "### The relation between Entropy and Perplexity",
    "[-0.03084254 -0.06585751 -0.03066626  0.05601094 -0.04695394 -0.02225033\n  0.032517   -0.01379184  0.11265042 -0.00362104]": "It turns out that *perplexity* is related to the concept of *entropy*",
    "[ 0.02456382  0.07262612  0.05565747 -0.00115398 -0.00562404 -0.00843582\n  0.15045385 -0.01529559  0.04199777 -0.03860186]": "The [[Entropy]] $H$ of $X$ is equal to the negative sum of the probability of $x$ times the $\\log_2$ of the probability of $x$, mathematically:\n$$\nEntropy := H(X) = - \\sum_{X \\in \\mathcal{X}} p(x)\\log_2p(x)\n$$\n\t Notice that the log can be computed in any b",
    "[ 0.05042729 -0.02333594  0.01092043 -0.02217118  0.00521715  0.05830926\n  0.04239022 -0.01186215  0.12277715  0.00102499]": "From *Speech and Language Processing*: \"One intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme",
    "[-0.04967382  0.08760084  0.0091056   0.03473666  0.04570086  0.02949063\n  0.19824763  0.00257543 -0.03022706 -0.10353064]": "\"",
    "[ 0.04888658  0.03438857  0.01064416  0.01745545 -0.01118852 -0.07341868\n  0.08237857 -0.01232131 -0.02281844 -0.00478412]": "For a better understanding of this formulation, let's say we have an opaque bag containing 64 marbles",
    "[ 0.023386   -0.00441931  0.056842    0.01295264 -0.08398204  0.04158521\n  0.01921254 -0.00680576 -0.02520072 -0.00639147]": "The marbles can be of 8 colors",
    "[ 0.05137583  0.02659106  0.02555866 -0.01770557 -0.04036602 -0.03821669\n  0.12663229 -0.04628303  0.02778743 -0.00912715]": "In the worst case, if the marble distribution is uniform (there are 8 marbles of each color in the bag), and we need to announce the result of pulling one marble from the bag using bits, we would always need 3 bits: since there are 8 colors in our palette ",
    "[ 0.04169551  0.02775485  0.00277554  0.02597717  0.025248   -0.03345928\n  0.08864421 -0.0360424   0.04108241  0.01279854]": "*Example*: But imagine that we know that in the bag there are 32 red marbles, 16 blue ones, 8 yellow ones, 4 green ones and only of each other color",
    "[ 0.03330331  0.06790392 -0.01980159 -0.00432102 -0.05050361 -0.12145039\n  0.01559105 -0.02844597 -0.05232391 -0.01623567]": "What we can do is reduce the *average* amount of bits needed to encode the result of pulling a marble out of the bag",
    "[-0.00266947  0.01268523  0.00929887 -0.0653163  -0.07622291 -0.00617351\n  0.05347434  0.00086039 -0.02315839 -0.01430291]": "If we let $0$ signify red, $10$ signify blue, 110 signify yellow, and so one to 11111110 signifying cyan, if we were forced to announce all 64 marbles, on *average* we would only need 2 digits",
    "[-0.05244    -0.00596675 -0.03073835 -0.02989101 -0.08453841 -0.00799167\n  0.02340632 -0.02583915 -0.00530468 -0.04083703]": "Sure, we can be unlucky and pick a cyan marble and have to use 8 digits, but half the time we would only need 1 for red",
    "[ 0.02463087 -0.01558654  0.04745543 -0.00598747 -0.06007183 -0.05091134\n  0.04077427 -0.07024275  0.05996863  0.01936467]": "If we plugged the formula for entropy assuming that the probability of each marble is the same (following an uniform distribution), we would get the result of 3",
    "[-0.003238   -0.04166627  0.02231094  0.00759061 -0.12661783 -0.10195128\n  0.01303667 -0.01024743  0.03983117 -0.00335235]": "If we did the same for our marble example, we would get 2",
    "[-0.06208662 -0.02331961  0.02650178  0.01795354  0.01325114 -0.02316518\n -0.03727906  0.00146805  0.15783377  0.0382968 ]": "Now, we can intuitively think that the higher the entropy of a distribution, the more difficult it is to efficiently compress it, and vice-versa",
    "[-0.05918428  0.06171204  0.05481547  0.03133511 -0.05295626  0.04838821\n  0.09630638 -0.03776985  0.10060734 -0.05343266]": "This gives us a first intuition about the relation between $PPL$ and $H$",
    "[ 0.02047318 -0.02962026  0.04109012  0.02780045  0.0135753  -0.07376241\n  0.00342902 -0.01496609  0.03435886 -0.00021944]": "But notice that in the marble example, we were only taking one marble at a time, and each marble was a sample",
    "[-0.08121674 -0.06025734 -0.02829244 -0.02985758 -0.08040721  0.09139869\n  0.01063452 -0.03540463  0.09874204 -0.03761801]": "But in the language modeling task, we are concerned with ***sequences***",
    "[-0.01414597 -0.00262725  0.02248735  0.0198334   0.01854095  0.07558846\n  0.07611291 -0.0689527   0.04307412 -0.04595294]": "The entropy of a random variable that ranges over all finite sequences of length $n$ of words, from a language $L$, is given by:\n\n$$\n\\text{Entropy for sequences} = H(w_1, w_2,",
    "[-0.0467952  -0.06171484  0.03149863  0.03868244  0.04724021 -0.05138135\n  0.06581172  0.02597663  0.06187088 -0.05149267]": ", w_n) = - \\sum_{w_{1:n}\\in L} p(w_{1:n}) \\log_2 p(w_{1:n})\n$$",
    "[-0.01160879 -0.06760323  0.01447085  0.03823695 -0.00281526  0.01401942\n  0.04828916  0.00523608  0.13180935 -0.02434062]": "Now we can define the *entropy rate*, which can be thought of the ***per-word entropy***:\n$$\n\\text{Entropy rate} = \\frac{1}{n}H(w_{1:n}) = - \\frac{1}{n} \\sum_{w_{1:n}\\in L} p(w_{1:n}) \\log_2 p(w_{1:n})\n$$",
    "[-0.03713772  0.05133141  0.07413255  0.00391458  0.01836127  0.15345217\n  0.06517641 -0.05763174  0.09317635 -0.04806088]": "One of the problems with this formulation is that in order to have the entropy of a language $L$ we need to consider sequences of infinite length, and the final formulation of $H(L)$ involves $n$ ranging to a limit, meaning we have to sum over all sequence",
    "[ 0.04993089 -0.06462741  0.00551818 -0.00894033 -0.00175544  0.01683391\n  0.06185244 -0.03870543  0.10767673 -0.03828248]": "But through the [[Shannon-McMillan-Breiman theorem]], if we assume the language to be *stationary* and *ergodic*, we can take a long enough sequence and compute its average log probability",
    "[ 0.05829651 -0.00859443  0.0447747   0.05162862  0.01320068  0.01428486\n  0.08448008 -0.02942921  0.07373919 -0.00827257]": "*WARNING*: natural language is **NOT** stationary",
    "[-0.01646823  0.05871812  0.04355747  0.05175775 -0.02562668 -0.04124031\n  0.02472298  0.02418566  0.01256413  0.07715009]": "This is an incorrect but simplifying assumption that allows us to have an approximate result",
    "[-0.06615356 -0.03796037 -0.01488282  0.01090499  0.00532974 -0.00090124\n -0.03024373 -0.05657784  0.05933058 -0.04464123]": "We can convert from perplexity to cross entropy and vice-versa, according to [this](https://thegradient",
    "[ 0.01086336 -0.07555984 -0.04291477  0.00370795  0.02821472  0.08234829\n  0.01888352  0.04618404  0.05417119 -0.03016942]": "pub/understanding-evaluation-metrics-for-language-models/#fnref4)",
    "[ 0.01947432  0.02143501 -0.02177787 -0.01068942 -0.00539122 -0.02840519\n -0.02118115 -0.02923991  0.03942595 -0.04760631]": "Going back to our marble example, suppose we do not know the exact ratio of marbles per color, and we are drawing sequences of marbles from the bag",
    "[ 0.04710773 -0.02610948  0.00715094  0.01909396 -0.0191287  -0.05436717\n  0.10437636  0.00899164  0.0280548  -0.00600154]": "How do we try to guess how many of each marble are inside the bag? That is, how do we estimate the real probability of the different colors in the bag?",
    "[-0.10299694 -0.01210626 -0.01526068 -0.01654674  0.08919947  0.06636457\n  0.06200625 -0.03352343  0.09082209 -0.01610095]": "The **[[cross-entropy]]** is useful when we do not know the exact probability distribution that generated a certain sequence",
    "[ 0.01769333 -0.04452571 -0.0134032   0.05451382 -0.00272699  0.01577807\n  0.01657452  0.02304438  0.06492039  0.05774369]": "Now assume $m$ to be some model of $p$, the real probability distribution",
    "[ 0.01069534 -0.07530632  0.04212821  0.03172722  0.06245413  0.0482319\n  0.02617959  0.01625484  0.03959288 -0.06478088]": "$$\n\\text{Cross-entropy} = H(p,m) = \\lim_{n \\rightarrow \\infty} - \\frac{1}{n} \\sum_{W \\in L} p(w_1,",
    "[-0.02871427 -0.01824621 -0.04896726  0.08547226  0.00727019 -0.04755793\n  0.06818943  0.03988141  0.02120396  0.00644286]": ", w_n) \\log m(w_1,",
    "[-0.05757168 -0.00116929 -0.01827269  0.07136418 -0.10532924  0.0092007\n  0.09442004  0.0146255  -0.01854801 -0.04059861]": ", w_n)\n$$",
    "[-0.02285927  0.00144604  0.03356832 -0.01318506 -0.02081322 -0.01009973\n  0.10504963  0.02566922  0.0612637   0.03644215]": "Now, we draw sequences according to the probability distribution p, but sum\nthe log of their probabilities according to m",
    "[-0.01832478 -0.0556499  -0.00882296  0.04049955  0.01921931  0.01922847\n  0.04802137 -0.01410974  0.04632948 -0.04664217]": "But this is still a summation, but we can use the Shannon-McMillan-Breiman theorem to consider a *long enough* sequence, and calculate:\n$$\n\\text{SMB Cross-Entropy} = H(p,m) = \\lim_{n \\rightarrow \\infty} -\\frac{1}{n} \\log m(w_1,",
    "[ 0.04066391 -0.0466878   0.05814227 -0.02537921  0.02839233  0.09725619\n -0.04792608  0.04930275  0.03481834 -0.04890909]": "The **Cross-Entropy** gives us an upper-bound on the entropy, such that for every model $m$, it is the case that $H(p) \\leq  H(m,p)$",
    "[ 0.00714843 -0.04207341  0.03432164  0.08046998  0.03576367  0.04518714\n -0.01787329  0.05113634  0.08519129  0.02665897]": "A corollary of the above is that a model $m$, no matter how bad it is, can never underestimate the real entropy $H$ of $p$",
    "[ 0.02942884 -0.02000458  0.00381062 -0.01138771  0.02714629  0.04646781\n -0.00681795  0.05412479  0.06050795 -0.04048287]": "The more accurate $m$ is, the closer the cross-entropy $H(p, m)$ will be of $H(p)$",
    "[ 0.04556037 -0.02724499  0.03906223 -0.00356265  0.04697112 -0.04784654\n -0.09933729  0.0679806   0.05540463 -0.05495236]": "**Finally: given two models $m_1$, $m_2$ we say $m_1$ is more accurate than $m_2$ iff $H(p, m_1) < H(p,m_2)$",
    "[-0.00769039 -0.07761139  0.04843747  0.01856884  0.0533283  -0.02919431\n  0.01295072  0.03361648  0.06898282 -0.05306578]": "That is, if the cross-entropy of $m_1$ with respect to $p$ is LOWER than the the cross-entropy of $m_2$ with respect to $p$, and vice-versa",
    "[-0.07255203  0.02415908  0.03909273 -0.00530607 -0.01403063 -0.02407634\n  0.13758545 -0.01023637 -0.00157556 -0.03210489]": "**",
    "[-0.09290198 -0.07447549  0.00040098  0.02273732  0.02013673  0.03619104\n -0.05255869 -0.02778507  0.0817904   0.01106875]": "Now, we still do not have access to a theoretically long sequence in the limit, but we can approximate it by fixing a long enough sequence of words $W$, and for a model $M = P(w_i \\",
    "[-0.0832774   0.07589284  0.02044404  0.05201581 -0.01282457 -0.02451577\n  0.12029358 -0.03338479 -0.01579449 -0.0728256 ]": "w_{i-N",
    "[-0.0342392   0.00754998  0.09637764 -0.00150345  0.04585596  0.0666866\n  0.08077684  0.05243969  0.05311923 -0.02835127]": "1:i-1})$  giving us the final formulation of approximate cross-entropy and that of perplexity:\n### Final formulation of the Perplexity of a model\n$$\n\\text{Approx",
    "[-0.05502165 -0.03228236  0.01217566  0.08209096  0.0360959  -0.03990554\n  0.05354054  0.00654751  0.05542989 -0.0510461 ]": "Cross-Entropy} = H(W) = -\\frac{1}{N} \\log P(w_1,",
    "[-0.05885393  0.05056835 -0.06579539  0.1101613  -0.09264677  0.01179128\n  0.08636229 -0.00520165  0.03562401 -0.03191506]": ", w_n)\n$$\n\n$$\nPPL(W) = 2^{H(W)}\n$$",
    "[-0.01766647  0.00730031  0.06848502  0.00170274  0.00952431  0.12736632\n  0.070391   -0.02812822  0.07140171 -0.02503022]": "Summary: 1) In order to theoretically calculate the entropy $H(L)$ of a language $L$ we need to know the probability distribution $P$ of all the finite length sequences of $L$",
    "[-0.04356469  0.00185993 -0.05116581  0.0610376  -0.06312867 -0.06969191\n  0.14290313  0.04640443 -0.02819013 -0.07125492]": "1",
    "[-0.13259897 -0.01185324 -0.04308998 -0.03048142  0.00570233  0.10346507\n  0.06082464 -0.03239214  0.052626   -0.04509955]": "1) This poses a problem because it means we need to consider infinitely many sequences",
    "[-0.02806856 -0.01778042 -0.0050156  -0.02271036  0.01536778  0.08185221\n  0.04024964 -0.08309904  0.0947177  -0.05189438]": "2) Due to the Shannon-McMillan-Breiman theorem instead of summing over all these infinitely many sequence, we can only consider a sequence of $len(n)$ as a $n$ approaches the limit",
    "[-0.02816484 -0.02478657 -0.09065127  0.04615798 -0.03430003 -0.07164945\n  0.0987462   0.05291064 -0.00465414 -0.0568627 ]": "2",
    "[-0.07618613  0.06243801  0.02334648  0.01711558 -0.00602451  0.00777675\n  0.03975796  0.02149563 -0.04470535 -0.04276817]": "1) This also needs an approximation, and for that we fix an $n$ that is very long",
    "[-0.03257057  0.02347465  0.0617832   0.05787843  0.00560737 -0.00797782\n  0.06497797  0.03899974  0.00810546  0.03356275]": "3) In order to calculate this approximation, we still need to *know* the probability distribution behind $W$, which in the real world is impossible to know",
    "[-0.02898987 -0.03105901 -0.09450375  0.02229462 -0.01062467 -0.0311977\n  0.09344777  0.01900542  0.00498646 -0.06396307]": "3",
    "[-0.13933124 -0.09036496  0.02004776  0.0378154   0.05555369  0.04219359\n  0.09645805 -0.00500151  0.0813003   0.00198922]": "1) In order to predict this probability distribution, we use the idea behind the Cross-Entropy, which sums over the probability of a certain sequence times the log of the probability of the same sequence given by a model $m$",
    "[-0.03903043 -0.00757914  0.03163048  0.03690946  0.05009207  0.08927962\n  0.06867162  0.03670612  0.04638996 -0.03457168]": "2) we then fix a sequence length of $n$ and take the cross-entropy of $p$ with respect to $m$ according to the formula above, and finally 4) we exponentiate 2 by the approximate cross-entropy $H(W)$ as in the formula above",
    "[ 0.00463722 -0.0005112   0.06044583  0.00401022 -0.03050082  0.07497322\n  0.06546735  0.00471116  0.0508979  -0.07464468]": "### Bounds of Perplexity, Entropy and Cross-Entropy",
    "[-0.00656176 -0.06991814 -0.00695984 -0.0033974   0.04330941  0.0462005\n -0.06098551 -0.00049961  0.08359767 -0.04257115]": "If we know the cross-entropy/perplexity of a certain model, we do not know how far it is from the optimal cross-entropy, but we know that it does not understimate the real entropy",
    "[ 0.04749143  0.01404925  0.01028574  0.03647096 -0.05498228  0.10095146\n  0.09257234 -0.08178893  0.13140957 -0.06261851]": "For a language consisting of only one symbol, the entropy can be 0",
    "[ 0.02432951  0.00216757  0.01654159  0.04871042  0.02144249  0.03433716\n  0.08658515  0.00354587  0.10355653 -0.0023068 ]": "For all others, the minimal entropy is 1",
    "[ 0.00883626 -0.03287384  0.06922147 -0.00244498  0.11127751  0.05903142\n  0.01543114  0.0429832   0.07285536 -0.06738327]": "Context-length matters for estimating the cross-entropy of a model",
    "[ 0.05985672 -0.0574454   0.03302628 -0.07546432  0.08262569  0.01045192\n -0.0550852   0.04631788  0.09163781 -0.05247703]": "If a model has a larger context length, then it should have lower cross-entropy compared to a model with smaller context length, *coeteris paribus*",
    "[-0.14282836  0.0724365   0.00330307 -0.01175033  0.03166304  0.01843243\n -0.02799246  0.01245152 -0.0914884  -0.00448825]": "The [[Transformers]] was introduced in the paper *Attention is all you need (2017)*",
    "[-0.03306958 -0.06913259  0.00798268 -0.00524765 -0.01261563  0.07399942\n  0.03048915  0.03727712  0.0453803  -0.05180461]": "It solves many problems associated with the language modeling task, improving on RNNs and LSTMs by introducing a mechanism called self-attention, which allows it to learn the dependencies between words in a sequence without running into the bottlenecks of ",
    "[-0.02527903 -0.04172023  0.00643932  0.01274771 -0.01912786 -0.00865707\n  0.01403477  0.03738267  0.0597167  -0.0123198 ]": "Self-attention is only a part of the Transformer model, which also uses skip connections, BatchNorm, and standard MLPs as parts of its architecture",
    "[-0.12126836 -0.05285517 -0.02330716 -0.02003234  0.02313819 -0.06732918\n -0.02173709 -0.06090151 -0.00637002 -0.02832625]": "The classic architecture in the 2017 paper includes an Encoder-Decoder structure, but modern GPTs ditch the Encoder part and only use the decoder in order to generate words given a previous set of words",
    "[-0.08419555  0.03651971  0.01386935 -0.02731307 -0.0841301   0.07307231\n -0.05556475  0.03259295 -0.00197785  0.00683029]": "Transformers solve a few critical issues when dealing with NLP: 1) variable input length by being able to take sequences of any size, 2) explosive number of parameters by introducing parameter sharing in the form of the Q, K, V matrices",
    "[ 0.03420509 -0.01627642  0.00372896 -0.02265258 -0.0002225   0.04427268\n  0.03176201 -0.03789899  0.04590988 -0.01675749]": "3) weakening of connection between distant words by relating all words to each other according to the input itself",
    "[ 0.06037258  0.03188634  0.0398556   0.03296536  0.04806099 -0.0112725\n  0.19612005  0.03386192  0.06457371 -0.07707093]": "### [[Self-attention]]",
    "[-0.0639701   0.01322436 -0.04950983 -0.04414179 -0.0235598   0.00413445\n  0.04638025  0.11894103  0.02432183  0.02905838]": "The main innovation of Transformers was the Self-Attention mechanism",
    "[ 0.01407658 -0.06575024 -0.02973633 -0.05522782 -0.01623467 -0.04215277\n  0.12748648  0.04302055  0.07041057 -0.0473238 ]": "One can play around with different metrics, but [[Dot-product]] self-attention is usual",
    "[-0.02061561 -0.00611535  0.06803728 -0.00044175  0.01629314  0.08391939\n  0.04672864  0.02786981  0.01701696 -0.00539062]": "### Chapter VII",
    "[-0.04122932  0.03559268  0.08033947  0.02952867  0.05830704 -0.05252315\n  0.00652679 -0.00883073 -0.01884842 -0.01939344]": "It had grown uncommonly sultry and dark when at midday, after resting on the beach, I climbed to Dunwich Heath, which lies forlorn above the sea",
    "[ 0.07166737  0.10696304  0.077306    0.06313015  0.03395782 -0.02156067\n -0.09077202 -0.0193777  -0.02589318 -0.01295421]": "The history of how that melancholy region came to be is closely connected not only with the nature of the soil and the influence of a maritime climate but also, far more decisively, with the steady and advancing destruction, over a period of many centuries",
    "[ 0.06242686  0.00707556  0.10172093  0.04525415  0.08115844 -0.05390935\n -0.11818174 -0.05508761 -0.08050055  0.02285413]": "In Norfolk and Suffolk, it was chiefly oaks and elms that grew on the flatlands, spreading in unbroken waves across the gently undulating country right down to the coast",
    "[-0.00935684  0.01122017  0.11256348  0.08628733  0.07520054 -0.06276831\n -0.12632284 -0.04692211 -0.04883859  0.10963596]": "This phase of evolution was halted when the first settlers burnt off the forests along those drier stretches of the eastern coast where the light soil could be tilled",
    "[ 0.02729862  0.0652041   0.11309001  0.06584135  0.1179195  -0.08690406\n -0.05887906 -0.03520648  0.00832639  0.06961076]": "Just as the woods had once colonized the earth in irregular patterns, gradually growing together, so ever more extensive fields of ash and cinders now ate their way into that green-leafed world in a similarly haphazard fashion",
    "[ 0.06527185  0.09881167  0.0312851   0.05493893  0.11027769 -0.03493866\n -0.00517647 -0.08929862  0.04994679  0.06020816]": "If today one flies over the Amazon basin or over Borneo and sees \u201cthe mountainous palls of smoking, hanging, seemingly motionless, over the forest canopy, which from above resemble a mere patch of moss, then perhaps one can imagine what those fires, which ",
    "[-0.01063859  0.14088324 -0.00241766  0.07383275  0.04507618 -0.01907647\n -0.01678621  0.00739453 -0.07071704  0.02258327]": "Whatever was spared by the flames in prehistoric Europe was later felled for construction and ship-building, and to make the charcoal which the smelting of iron required in vast quantities",
    "[ 0.08974313  0.02734229  0.03342754  0.0161726   0.18297502 -0.12412834\n -0.0763019  -0.05193752  0.02571356  0.09999491]": "By the seventeenth century, only a few insignificant remnants of the erstwhile forests survived in the islands, most of them untended and decaying",
    "[ 0.02667192  0.1000069   0.06033823  0.04617043  0.09117763 -0.01927831\n -0.06041927 -0.00157226 -0.02913232 -0.00957561]": "The great fires were now lit on the other side of the ocean",
    "[ 0.04259667  0.00181734 -0.02686117 -0.04410997  0.00628545 -0.009798\n  0.04431654 -0.03124233  0.02263868 -0.0123443 ]": "It is not for nothing that Brazil owes its name to the French word for charcoal",
    "[ 0.02834228  0.1319657   0.0308567   0.01877112  0.14496972 -0.01732447\n  0.04213766 -0.03310937 -0.06374755  0.06973786]": "\u201cOur spread over the earth was fuelled by reducing the higher species of vegetation to charcoal, by incessantly burning whatever would burn",
    "[-0.02069875  0.01968338  0.01419673  0.05963654  0.05518304 -0.02230566\n -0.01167776 -0.0072657  -0.01454535 -0.03263161]": "From the first smouldering taper to the elegant lanterns whose light reverberated around eighteenth-century courtyards and from the mild radiance of these lanterns to the unearthly glow of the sodium lamps that line the Belgian motorways, it has all been c",
    "[-0.04548755  0.05556805  0.03640939  0.06589799  0.12450821  0.0454401\n  0.00176179 -0.03151866  0.06909239  0.05171385]": "Combustion is the hidden principle behind every artefact we create",
    "[-4.7253486e-02  2.9630616e-02 -9.6063726e-03 -2.7544662e-02\n -2.4796121e-02  4.5026816e-02  4.9486051e-05  2.3499601e-02\n  1.0003699e-02 -2.5697136e-02]": "The making of a fish-hook, manufacture of a china cup, or production of a television programme, all depend on the same process of combustion",
    "[-0.00227406  0.02263047  0.03385227  0.08882649  0.00200614 -0.06911352\n  0.04647547 -0.06934272  0.09648061 -0.01628388]": "Like our bodies and like our desires, the machines we have devised are possessed of a heart which is slowly reduced to embers",
    "[-0.01704227  0.01981532  0.06819617  0.10609358  0.05499124 -0.06119585\n -0.02898198  0.00409217  0.04310171  0.05475032]": "From the earliest times, human civilization has been no more than a strange luminescence growing more intense by the hour, of which no one can say when it will begin to wane and when it will fade away",
    "[ 0.05082446  0.06697099  0.12673615  0.08344112  0.09536002 -0.03432735\n -0.02566385 -0.11130234 -0.02435677 -0.00645068]": "For the time being, our cities still shine through the night, and the fire still spreads",
    "[ 0.06309695  0.12310334  0.05904599  0.06176563  0.16017315 -0.04579069\n  0.01755437 -0.06477398 -0.03092274  0.04037721]": "\u201cIn Italy, France and Spain, in Hungary, Poland and Lithuania, in Canada and California, summer fires consume whole forests, not to mention the great conflagration in the tropics that is never extinguished",
    "[ 0.07378542  0.05064841  0.02496766  0.07691355  0.1815098  -0.08501612\n -0.0602922  -0.06935927 -0.05425086  0.08112661]": "A few years ago, on a Greek island that was wooded as recently as 1900, I observed the speed with which a blaze runs through dry vegetation",
    "[ 0.02723387  0.10461237  0.0208081   0.09053051  0.08770651  0.01903412\n  0.07170048 -0.05028321  0.00109222 -0.0460286 ]": "A short distance from the harbour town where I was staying, I stood by the roadside with a group of agitated men, the blackness behind us and before us, far below at the bottom of a gorge, the fire, whipped up by the wind, racing, leaping, and already clim",
    "[-0.0680543  -0.06399964  0.00919133  0.05375866 -0.01415821 -0.02107264\n  0.0714439  -0.03999391  0.05540152 -0.03688634]": "And I shall never forget the junipers, dark against the glow, going up in flames one after the other as if they were tinder the moment the first tongues of fire licked at them, with a dull thudding sound like an explosion, and then promptly collapsing in a",
    "[ 0.0162378   0.00993693  0.07572884 -0.01308358  0.03978157  0.07321373\n  0.08351178  0.02757571  0.03897997 -0.04208987]": "\u201d\n\n### Chapter VIII",
    "[-0.03102017  0.03941188 -0.01530046 -0.06093513  0.05322209  0.00349858\n  0.11530157  0.03571946 -0.02676727  0.01963379]": "\u201cWhen Quincey died, I no longer knew what to do",
    "[-0.05796033  0.09990487  0.02899427  0.02836719 -0.02609047 -0.00810939\n -0.01825625 -0.00718342 -0.00320336  0.00966593]": "First I sold the silver and china at auction, and then little by little the pictures, the books and the furniture",
    "[ 0.12207743  0.03552239  0.01777664  0.00097144  0.07143685 -0.02738267\n -0.02430587 -0.10011659  0.0055311  -0.02082662]": "But nobody ever showed an interest in taking on the house, which was getting more and more run down, and so we have remained tied to it, like damned souls to their place",
    "[-0.03801655  0.07336061  0.04824501  0.04533057 -0.05721912 -0.03495402\n  0.01359279 -0.17214116 -0.0474725   0.0807507 ]": "Whatever we have tried, from the girls\u2019 sewing to the nursery garden Edmund once started to our notion of having paying guests, has without fail gone wrong",
    "[ 0.04258543 -0.00882407  0.04198354  0.00585891  0.01971424  0.05913254\n  0.0330452  -0.06257756 -0.04134698 -0.09040399]": "You, said Mrs Ashbury, are the first guest who's ever found his way here in the almost ten years since we put the advertisement in the Clarahill grocer's window",
    "[ 0.01003854 -0.04559431 -0.01150804  0.00977797  0.01016559 -0.07768412\n  0.02742949  0.07650202  0.04227337  0.0140501 ]": "Unfortunately I am a completely impractical person, caught up in endless trains of thought",
    "[ 0.0658436  -0.07681675  0.04257431  0.00610664  0.06624018 -0.03154002\n  0.06211168  0.02761027  0.1177874   0.06941069]": "All of us are fantasists, ill-equipped for life, the children as much as myself",
    "[ 0.03455161 -0.04542867  0.0995566   0.05699067  0.12155733 -0.00490983\n -0.02729678 -0.06146487  0.0840824  -0.05974777]": "It seems to me sometimes that we never got used to being on this earth and life is just one great, ongoing, incomprehensible blunder",
    "[-0.046541    0.07769291  0.03462382  0.02182536  0.05588938  0.00024742\n  0.19718082 -0.00593368 -0.01933672 -0.09597477]": "\u201d",
    "[-0.08250704  0.08389056  0.07987844  0.03456664  0.02293476  0.02118366\n  0.09869495  0.01029033 -0.10200936 -0.03002052]": "# 21/05/24\n\n### ==Todo==\n- [ ] Edit extended abstract for DL",
    "[-0.00431194  0.00074178  0.04927189 -0.00265355  0.05760936 -0.0484325\n  0.04653611 -0.05311518  0.02702151 -0.01506124]": "- [ ] Send e-mail to Box2EL authors",
    "[ 0.07017495 -0.01967316  0.06113591 -0.0059178   0.01649268 -0.06656419\n -0.01640986 -0.03097838 -0.0223076  -0.05356455]": "- [ ] Try to fit ontology to Box2EL'\n- [ ] Add tail corruption",
    "[ 0.0316689   0.0587214   0.02875875 -0.05348182  0.02434275  0.00234765\n  0.11550163  0.02915786  0.00702543 -0.02161711]": "- [ ] Add tail evaluation",
    "[ 0.02856837 -0.0651184   0.02401024  0.01962128  0.04029509  0.03878115\n -0.01229373 -0.00494779  0.00273325  0.00679608]": "- [ ] Re-read canonical model",
    "[-0.01381503  0.08122361 -0.01085245  0.02072954 -0.04683115 -0.00261118\n  0.09399863  0.02097567 -0.01518323  0.00808651]": "- [x] Edit code",
    "[ 0.03208547  0.06392    -0.00104987  0.01664358  0.01895414 -0.03956003\n  0.06519085  0.02737026 -0.00220467 -0.00322782]": "- [ ] Study",
    "[-0.01969143  0.01828886  0.09361677  0.04440284  0.0819543  -0.02266979\n  0.10374604  0.00256571 -0.03439803 -0.03019544]": "# 27/05/24\n### ==Todo==\n- [x] Send e-mail to Box2EL",
    "[ 0.03451414 -0.0006784  -0.00275783 -0.03038646  0.00013405  0.01670141\n -0.01035847  0.02840539 -0.02598687  0.0117015 ]": "- [ ] Study 2-3 chapters",
    "[ 0.05521625  0.02933062  0.01340456  0.00869495  0.08105042  0.02619222\n  0.104054   -0.04531018  0.0052607  -0.00988884]": "- [x] Separate concept loss from role loss",
    "[-0.00460508  0.01379223 -0.0356985  -0.0159924   0.06391808  0.01299527\n  0.10042131 -0.01275198  0.0019785  -0.04410934]": "- [x] Separate scoring function",
    "[-0.05931441  0.01775592  0.07682333 -0.05352874 -0.07227384 -0.09839041\n  0.08730041 -0.01919302 -0.05418359 -0.05763475]": "- [ ] Fix NegSampling",
    "[-0.04573845  0.01580341  0.03785355  0.06023062  0.09669099  0.01605383\n  0.11396912  0.07158527 -0.04726816 -0.00324438]": "# 28/05/24\n### ==Todo==\n- [ ] Try prime individuals (URGENT)\n### ==Thoughts",
    "[-0.0725215  -0.05173732 -0.02932602  0.01080752 -0.07816949 -0.03728316\n  0.04668856  0.06802456 -0.02478537 -0.0219697 ]": "I've been toying with gamma and phi values, and it seems to give good results",
    "[-0.02233774 -0.04966262 -0.00853656 -0.01068372 -0.05646131 -0.04590079\n -0.01400335  0.03191196 -0.03112796 -0.02874056]": "I have tried very high phi and 0",
    "[-0.07425109 -0.03502664 -0.00869577 -0.04665484 -0.01888    -0.02315533\n  0.04956829  0.09379338 -0.06176918 -0.00544388]": "5 gamma, with a large LR and this has been good",
    "[ 0.00613924 -0.02630454  0.00996907  0.00667987  0.03504396 -0.00298153\n -0.09802508  0.01099561  0.06390804  0.0101578 ]": "It seems the more we constrain the model geometrically, the worse it is on roles",
    "[-0.05497149  0.03642219  0.03013862 -0.05650685  0.08107813 -0.00777322\n  0.17111051  0.07050975 -0.04036555  0.02182116]": "# 30/05/24\n### ==Todo==\n- [x] Camera Ready DL",
    "[-0.0210944   0.10265221  0.03803148 -0.0644832   0.01045135 -0.012704\n  0.1016987  -0.02137574 -0.0392718  -0.01434991]": "- [x] Abstract RuleML",
    "[ 0.03923334  0.06945407 -0.01329859 -0.00381418 -0.00378699  0.04045331\n  0.02514909  0.03279877 -0.02214521  0.02125873]": "- [x] Study Ch4, 5, and 6",
    "[-0.00267883  0.00657497  0.04114405 -0.06814253  0.06073848 -0.05783146\n  0.13272667  0.01974573 -0.01528502 -0.00876876]": "- [x] Mess around with the Top concept",
    "[-0.0271683  -0.0223988   0.02204656  0.05631569  0.03493114  0.03729175\n  0.10268409  0.04686753  0.03048981 -0.0323696 ]": "# 03/04/24",
    "[-0.05755642 -0.05428221  0.03011806 -0.04299093 -0.00552855  0.02596063\n -0.02660909  0.06860091  0.03483526  0.04860188]": "I am convinced that the creation of the canonical model, the implementation of the function $\\mu$, and the implementation of geometric interpretation are correct",
    "[-0.02395942  0.01927934 -0.06141614 -0.08584571 -0.03175509  0.01229419\n -0.07902116  0.00106383  0.02242739  0.02582933]": "I am convinced the centroid has been implemented correctly",
    "[-0.04679104  0.00930473 -0.00046845 -0.0155472  -0.00771658  0.03744078\n  0.02372763  0.02629177  0.0629945  -0.01247093]": "Now I need to play with different loss functions",
    "[-0.03341378  0.01049708  0.0642756   0.05211801  0.03966935  0.08665159\n  0.03630266  0.01882312 -0.04975593 -0.03051549]": "### ==Potential issues and things to remember==:",
    "[-0.01744781  0.06724913 -0.11055787 -0.00573944  0.03309162 -0.0038336\n -0.07925225 -0.03459625 -0.02257184 -0.01999297]": "The role hak evaluation only works for predicting the object of the triple (s, v, o)",
    "[-0.02415439 -0.00891603  0.06737212 -0.05248193 -0.03394553 -0.01318076\n  0.08101592  0.03681026  0.02462097  0.01377263]": "The elements for $A \\sqcap A$ are not being initialized correctly (they are filled with 0s) when at least $\\mu(d)[A]=10$ should be the case",
    "[-0.08498371  0.01970625 -0.00392926  0.05710213 -0.0030479   0.04418973\n -0.00204555  0.04137782  0.06651123  0.00843573]": "The model",
    "[ 0.03867669 -0.04077766  0.0126813  -0.00401958  0.05055608 -0.05499095\n  0.08033808 -0.01980796 -0.04907488  0.01114115]": "concept_embedding_dict",
    "[-0.0356697   0.0876604  -0.07154351  0.11589146 -0.04546812 -0.01304788\n  0.14972936  0.02029344 -0.03057126 -0.0491265 ]": "weight[0]",
    "[-0.02172951  0.02403961  0.03335361  0.01430445 -0.00102864 -0.02563344\n  0.00158277 -0.01060056  0.04240955 -0.01237217]": "shape == d while the model",
    "[ 0.02214786  0.04570914  0.03811242 -0.01321264  0.01919569 -0.03732961\n  0.04965815 -0.03752172 -0.06183356 -0.08148585]": "role_embedding_dict",
    "[ 0.04930953  0.03957742 -0.01675196  0.01535483 -0.09115557 -0.08395053\n  0.00200158 -0.00595329 -0.00897393 -0.04740221]": "shape == 2d",
    "[ 0.0358826   0.01703484  0.0547801  -0.0220076   0.051518   -0.03786046\n  0.04745923 -0.02689155 -0.01461603 -0.05647031]": "individual_embedding_dict",
    "[-0.01474517  0.08066216  0.02083694 -0.01332779 -0.02838811 -0.05640734\n  0.07093366 -0.01609281 -0.01244514 -0.00578982]": "shape == d (This is correct, and should be kept like this)",
    "[-0.03825019  0.06450628  0.06811409  0.0410082   0.02952745 -0.0401596\n  0.02769274 -0.03481181 -0.04901668 -0.00516178]": "Testing \n\n### ==Tests done today==",
    "[ 0.02244174 -0.04368797  0.074514    0.01746033  0.06049826 -0.05686237\n -0.02200513 -0.01195913 -0.09624714 -0.01783009]": "I got rid of the training for concepts and massively reduced the X_roles variable (effectively reducing the amount of samples in the role training set) and the model was able to overfit on training data for len(X_roles) == 5",
    "[ 0.06880881  0.03121985 -0.01570501 -0.06527343 -0.04230522  0.00228203\n  0.10165904  0.02238495 -0.06857809  0.0024241 ]": "Already for len(X_roles) == 100 it fails",
    "[-0.08040527  0.02889038  0.0479237   0.00721922  0.04441196 -0.07574017\n  0.04729765  0.04804527 -0.00529016 -0.00137252]": "Why?",
    "[ 0.01841597 -0.06097819  0.00515674  0.00366124 -0.01806438  0.00575159\n  0.0199081  -0.06617102 -0.00075197  0.02674865]": "I will try to fix the moving parameter for roles and let the entities themselves work out the path",
    "[ 0.00515252  0.00093657  0.06651121  0.02699809  0.06236798  0.02375575\n  0.11445234  0.05803999 -0.03467706 -0.03744587]": "# 04/04/24\n### Todo",
    "[-0.02405199  0.00428659 -0.04721002 -0.04694674  0.0384076   0.06094911\n  0.09341578 -0.070064   -0.00385498 -0.03302196]": "~~Restrict language to only exists_r",
    "[-0.07492106  0.02323792  0.00909981 -0.02852601 -0.01494363 -0.03147497\n  0.04685643 -0.02907917 -0.01299504  0.04162105]": "T (Mostly done, but still a bug when creating vector dimensions)~~ ==Not even worth looking into, helps nothing",
    "[-0.06733771  0.02473181  0.07385965  0.01719762 -0.05541891 -0.05419866\n  0.09220729 -0.02874474  0.00093124 -0.01315345]": "==",
    "[-0.05084206 -0.01878615 -0.06047083 -0.07200293 -0.0063794  -0.03597937\n -0.04735277  0.00974615  0.02825492 -0.06789866]": "Try a cosine similarity loss function",
    "[ 0.00337409 -0.0396909   0.02483736 -0.01848266  0.02311213 -0.0047887\n -0.01109329 -0.01999611  0.05543768 -0.0056978 ]": "Try a contrastive loss function",
    "[ 0.04401185 -0.0202081   0.01138174  0.02456494 -0.0224399  -0.01885223\n -0.04670985 -0.02497667 -0.05623479 -0.01843276]": "Test the dataset using BoxE, BoxEL, or Box2EL",
    "[-0.03849383  0.07342545  0.07198744  0.0439338   0.02948964 -0.0452584\n  0.03346943  0.00241513 -0.05103508 -0.00107277]": "### ==Tests done today==",
    "[ 0.02487881 -0.05269916  0.00151153  0.02599177  0.0132679  -0.01900319\n -0.00555318 -0.07720648 -0.05246561  0.03110605]": "Debugged the hits@k for roles extensively, found nothing wrong",
    "[ 0.0365065  -0.05221388 -0.05051618  0.00993376  0.02554149 -0.0145342\n  0.07748628  0.01380568  0.08236715  0.04177061]": "Same with hits@k for concepts",
    "[-0.03700902 -0.01436098  0.02669155 -0.03751472 -0.04111413  0.08187938\n  0.00312765 -0.06333674  0.0458502   0.00726567]": "Current goals: restrict the language, make the model work with the cosine similarity loss function, fix the problem with the $A \\sqcap A$ elements",
    "[-0.02488858  0.13386597  0.11324272 -0.03258933  0.01839219  0.03200975\n  0.10269021 -0.02856152 -0.04701226 -0.02164374]": "### ==Positive things done yoday==",
    "[-0.06133093  0.09242966 -0.02571741 -0.04096735 -0.01480893  0.03847045\n  0.02188276  0.01205713 -0.10640534 -0.03523925]": "Correctly implemented the restricted language version of the code",
    "[-0.04992291 -0.03157936  0.02188476  0.00514812  0.01167266 -0.01085651\n  0.01346169 -0.07072575 -0.01715968  0.04294408]": "Tested the model in its original form on X_concepts without assertions of the thing (Thing, x) or (Q5, x), both in the restricted language and full language",
    "[ 0.03251856 -0.00070627  0.02119702 -0.01711175 -0.01326018  0.00454341\n  0.02613583 -0.0339559   0.0678677  -0.00263058]": "It sucks either way",
    "[-0.02542048 -0.00477268  0.05526137 -0.05414168  0.09139118  0.02249396\n  0.14565854  0.04703873  0.04502455 -0.01820878]": "### Thoughts from today",
    "[ 0.01391094 -0.07847427  0.01651421 -0.04506511 -0.04286363  0.02042534\n -0.01980408 -0.09940866  0.05235177 -0.02460721]": "What if the real problem is that I am training the entities in a space $\\mathbb{R}^d$ and not the full space $\\mathbb{R}^{2d}$? A possible solution is to train everything in $\\mathbb{R}^{2d}$ and instead of concatenating, I can just use the first half of t",
    "[-0.02728127  0.00041172  0.03342252  0.0383725  -0.01162447  0.02016169\n -0.06280681  0.03195488 -0.00491267  0.03245666]": "Found a possible issue with the forward method of the model",
    "[-0.01000289 -0.02928197 -0.03456368  0.03838269 -0.02123016 -0.02668663\n -0.04160928  0.06851442 -0.10377406 -0.04324023]": "Check again the dimensions for batch, dimensions of the data\n\n# 05/04/24\n\n### ==Todo==",
    "[-0.03728772 -0.0035576  -0.05693039 -0.06340564  0.0348826   0.0273157\n -0.0308652  -0.00884934  0.03716021 -0.05671125]": "~~Try a cosine similarity loss function and adapt the scoring to match it",
    "[-0.0541796   0.00129809  0.0144011  -0.09539971 -0.01749581 -0.0089968\n  0.13148206  0.0101615   0.0226255   0.01523326]": "~~",
    "[ 0.0025537   0.05315972  0.03731457 -0.03159782 -0.03966421 -0.03405221\n  0.1065577  -0.01836401  0.03257674 -0.0072337 ]": "Remove the random call to GeometricInterpretation",
    "[-0.01360926  0.09599885 -0.03522959 -0.03602185 -0.03367355  0.04157058\n  0.0383122  -0.04769738 -0.01965106 -0.02589851]": "etc in the h@k function, for concepts and for roles",
    "[ 0.00397944  0.0922044  -0.09458347 -0.02801245  0.01764046  0.01883743\n -0.00239341 -0.00052438 -0.04915348 -0.015848  ]": "The role hak evaluation only works for predicting the object of the triple (head, r, ?), implement for (?, r, tail)",
    "[ 0.02818599 -0.0541454   0.04058032 -0.01223817 -0.03117419  0.05944655\n  0.00525832 -0.05105816  0.0338913   0.01582519]": "Fix the creation of $c_{A\\sqcap A}$ embeddings (the embedding vectors for intersections are being created with only 0s)",
    "[ 0.07586966 -0.05861038 -0.02571345 -0.01944781 -0.02491584 -0.01908892\n -0.07990361 -0.04228479 -0.05850938 -0.01663033]": "Try the family ontology dataset using BoxE, BoxEL, or Box2EL",
    "[ 0.0368804   0.05390709  0.07263855  0.03064594  0.0347495  -0.00691356\n  0.03459071 -0.07954842 -0.07925333  0.02026894]": "~~Contribute to the Aurora or at least read what has been done to prepare for the visit",
    "[-0.03354501  0.02044227  0.06600294 -0.05189031  0.03971334  0.01218375\n  0.15835558  0.06951617  0.0656589  -0.03227751]": "~~\n\n### ==Thoughts==",
    "[-0.01311387 -0.03795459  0.00774963  0.00624363 -0.06723242  0.08851214\n -0.02450153 -0.06036468  0.05230762  0.02156206]": "If statements of the type $r(a,b)$ can be rewritten as $\\{a\\} \\sqsubseteq \\exists r",
    "[ 0.03705664 -0.00317009 -0.00120111  0.05709021 -0.05596164  0.01822258\n -0.01243997 -0.09214678  0.09803836  0.01488157]": "\\{b\\}$, is it possible to leverage this in order to reduce the dimensionality of the FaithEL model? For example, we convert every individual to a concept name (this is potentially *very* large, but at least it is linear)",
    "[ 0.0157      0.0709074   0.10141031 -0.00664658 -0.01421346  0.03257246\n  0.14860104 -0.04120414  0.05714701 -0.11436217]": "[[FaithEL]]",
    "[ 0.00080467  0.03843802 -0.0044905  -0.0488021  -0.07888996  0.00903364\n  0.07502539  0.00632127  0.00596392 -0.06268508]": "Let $\\mathcal{B} = \\{emb(a) \\",
    "[-0.026446    0.05021567 -0.02612053 -0.01166334 -0.02561715  0.03895644\n  0.04874022  0.02234518  0.05769322 -0.04952214]": "emb(a) = c(C) \\text{ for all } C(a)\\}$ where $c(C)$ denotes the centroid of $\\eta_{\\mathcal{I}}(C)$, and let $c(\\mathcal{B})$ be the centroid of",
    "[-0.03449005 -0.0375879   0.04942287 -0.034597   -0.00546639  0.01251159\n -0.10913039 -0.0582579  -0.03326353 -0.0063715 ]": "I am starting to get worried that the problem is so simple that the final embedding can be seen as just a set of simple steps of linear operations, s",
    "[-0.05895756  0.05387326  0.0083168   0.02068046  0.002201   -0.03443443\n  0.08473178  0.10776821  0.05895734 -0.01663864]": "t",
    "[ 0.02709692 -0.02079634 -0.02063981  0.01793619 -0.0293752   0.05341272\n  0.0369967  -0.05025223 -0.00328623 -0.01666638]": "that the final embedding of the parameters for individuals, concepts and roles can be interpreted in this way:",
    "[-0.0422122   0.03518025 -0.0611558  -0.00928602 -0.03532128  0.09170917\n  0.04006261  0.0331262   0.00804254 -0.05470134]": "$emb(a)$ = $\\frac{1}{N} \\sum p(C)$ such that $C$ is in the training set",
    "[-0.06284358  0.11431765 -0.00121587  0.09084632 -0.02157989  0.03960271\n  0.05689869  0.03538578  0.08431529  0.03477046]": "$p(C)$ = $\\frac{1}{N} \\sum$",
    "[-0.08243974 -0.06351884  0.00978277  0.03412764  0.05033478 -0.03627601\n -0.07842238 -0.00064783 -0.02105973 -0.01730664]": "I'll (probably not) finish this later, but I'm 100% sure that unless we do some form of negative sampling or contrastive loss, there is nothing interesting about this model at all",
    "[-0.04497112  0.07991246 -0.00572044 -0.02827276 -0.03892625 -0.07731441\n  0.06013684 -0.00920696  0.02210983  0.01522218]": "Mental note: ==implement negative sampling==",
    "[ 0.02251506  0.04820615  0.1341041  -0.02067374  0.00855656  0.01874015\n  0.11906379 -0.04705659 -0.01573344 -0.00222466]": "### ==Activities done today==",
    "[-0.0987387   0.07545725 -0.00885616  0.06813647  0.00958529 -0.03116626\n  0.01231222 -0.0406772  -0.06672953  0.05180427]": "Reviewed Aurora paper",
    "[-0.01975037 -0.02219254  0.06175742  0.02949286 -0.02129865  0.02904924\n -0.02929158 -0.08797114 -0.05146831 -0.08030468]": "Removed previous alt_training routine to nn",
    "[-0.01447994  0.0657985  -0.0770833  -0.05787648 -0.06678527  0.06690598\n  0.04634689  0.01571833  0.001327   -0.02884549]": "CosineEmbeddingLoss()",
    "[-0.0615858   0.03715107 -0.03443603 -0.05564464 -0.0584149   0.0336412\n -0.0084897   0.00160844 -0.04555172 -0.0478797 ]": "Implemented the routine to train using CosineEmbeddingLoss() without negative sampling",
    "[-0.03360556  0.01939489 -0.05545168  0.00181022  0.00533096 -0.00680057\n  0.03103076 -0.03173583 -0.00902255  0.00086453]": "Watched this [lecture](https://www",
    "[-0.07637224  0.00063814 -0.0368188  -0.01428312  0.02375362  0.04571738\n  0.04979748 -0.06174239  0.05351304 -0.01226017]": "com/watch?v=dVH1dRoMPBc) by Sasha Rush on [[Mamba]], but did not fully understand it",
    "[-0.07907719 -0.05780178 -0.02003382  0.01925103  0.01177631  0.00385697\n -0.11689496  0.01067904 -0.01531536 -0.00802721]": "Might have to rewatch it after reading some more about the model",
    "[-0.01614024  0.14019506 -0.03212961 -0.07342202 -0.02978562  0.00828324\n  0.04369687 -0.01299829 -0.10092319 -0.00678264]": "# 07/04/24\n\n### ==Todo==\n- [x] Implement h@k evaluation for triples (?, r, t) since currently I only have (h, r, ?)",
    "[-0.03698153  0.13483505 -0.02847369 -0.08550225 -0.01210692  0.02409933\n  0.05844625 -0.00119616 -0.06723233  0.00938262]": "# 24/04/24\n\n### Todo\n- [x] Implement h@k evaluation for (?, r, t) triples",
    "[ 0.02729402  0.03317785  0.00696643 -0.04300362  0.03002847 -0.06988395\n  0.03549049 -0.02475763 -0.00017647 -0.01339966]": "- [x] Implement negative sampling naive\n- [ ] Compare with another model (principalmente Box2EL)\n### ==Thoughts==\n\n# 26/04/24\n\n### ==Thoughts==",
    "[-0.01708116  0.04343367  0.00573904 -0.00374954 -0.0018561  -0.02351975\n -0.07257495 -0.01062496 -0.01358275 -0.05780172]": "Tested 2000 epochs with eval every 10 epochs with centroid_score = True and phi = 0",
    "[-0.04131204 -0.00670327 -0.03135115  0.03367246  0.00300573 -0.00118032\n  0.08267044  0.01653075 -0.01819728  0.00195153]": "5",
    "[ 0.02347847 -0.02141363 -0.03034666 -0.0028432  -0.04896875  0.02215658\n  0.03346858  0.0991932  -0.00182367 -0.01589487]": "Hits@10 stabilize at 0",
    "[-0.02368508  0.01134405  0.06731929 -0.00953991 -0.00797493 -0.1320681\n -0.07105137  0.00659914 -0.05943029 -0.05868815]": "5 after roughly 1000 epochs, but reaches peak performance at around 500 epochs with 0",
    "[-0.00570963  0.01043063 -0.0480664  -0.00094465  0.00703085  0.07238527\n  0.03069356  0.08889332 -0.02009081 -0.07263207]": "9",
    "[ 0.06584607 -0.02690752 -0.05397786 -0.01031189 -0.01706524  0.02889995\n -0.08752697 -0.05207844 -0.02138864 -0.02614344]": "I need to train the model with the same settings but centroid_score = False to see if the performance goes up\n\n# 30/04/24\n\n### ==Todo==\n- [ ] Test the Family Ontology on Box2EL\n\n# 08/05/24\n### ==Todo==\n- [x] Fix Top to have its centroid on the origin 0",
    "[ 0.01019677 -0.00658302  0.00208851 -0.01292075  0.04593842  0.04147743\n  0.07588702 -0.06689429 -0.06756838 -0.04933783]": "- [x] Train the model on roles only",
    "[-0.01537709  0.07222892  0.04515281 -0.05139123  0.02299394 -0.09249768\n  0.14672592 -0.00256501 -0.00128455 -0.01652052]": "- [x] Fix the negative sampling",
    "[-0.02138423  0.01912745 -0.02353222 -0.07337794  0.0201514   0.01615539\n  0.09765369 -0.04976821 -0.02147277  0.0047563 ]": "- [x] Add support for the (?, r, t) triples on hits@k eval",
    "[-0.00525298  0.10518589  0.02359996 -0.01445124 -0.02586183 -0.09805305\n  0.10876353 -0.05856672 -0.07708947 -0.03534903]": "- [x] Clean up the code, removing instances of GeometricInterpertation from classes (spaghetti)",
    "[-0.04059362  0.06921136 -0.01194003 -0.01727423 -0.00363532 -0.0719866\n  0.04131445  0.00607916 -0.0178229   0.04304934]": "- \n\n# 07/05/24\n### ==Todo==\n- [ ] Mandar email p/ Box2EL ou conseguir testar na implementa\u00e7\u00e3o deles",
    "[-0.04436095  0.076068   -0.03323742 -0.03228097  0.02192599 -0.02626863\n  0.13586377  0.02415763 -0.06722061 -0.02382874]": "- [ ] Fazer exemplo na m\u00e3o\n\n# 21/05/24\n\n### ==Todo==\n- [x] Restructure code",
    "[-0.00318367  0.00507573 -0.03457688 -0.03906413  0.03169177  0.01772868\n  0.08493829 -0.063398   -0.05566404 -0.01589797]": "- [x] Add support for the (?, r, t) triples on hits@k eval roles",
    "[-0.01350865  0.00262272  0.07825696 -0.03572814  0.04031123  0.01110599\n  0.12550162  0.07144324  0.06526101 -0.05725079]": "### ==Thoughts===",
    "[ 0.04023292 -0.07881707 -0.03228754 -0.0167899  -0.06256061 -0.0416616\n -0.03274686 -0.00666792  0.01647659 -0.00437564]": "I am trying a few things",
    "[-0.00470109 -0.03214946 -0.11817064  0.05847286  0.01965354  0.01807164\n  0.02420279  0.07970528  0.06652663  0.01924283]": "First, I want to see how scoring w",
    "[-0.05814693  0.00076735 -0.07634652  0.06422824  0.00153892 -0.02440348\n  0.08758313  0.04512016  0.00578569  0.00517636]": "r",
    "[ 0.03182738 -0.00543804 -0.03738524 -0.0453896   0.00157689  0.0316188\n -0.09427086 -0.05177212  0.09052745  0.04139718]": "only to the centroid changes things",
    "[ 0.0493824   0.03395947  0.062536    0.07051977  0.03600709 -0.00855462\n  0.02632734 -0.00443275 -0.01261133 -0.05537545]": "# 22/05/24\n\n### ==Todo== \n- [ ] Try to use the data in Box2EL",
    "[-0.03808933  0.07778414  0.0817405  -0.02305033 -0.01215716  0.02582654\n  0.05277472 -0.04503266  0.03538794 -0.01344837]": "- [x] Change the loss function to only fix one part of the relation",
    "[-0.07864025  0.07231009  0.05263947  0.00930169  0.07132149 -0.04266996\n  0.10755736  0.03862882 -0.02234073 -0.04274406]": "# 23/05/24\n\n### ==Todo==\n- [ ] Make negative sampling into a helper function",
    "[-0.01916539  0.00935016 -0.0313388   0.01255583 -0.05145739 -0.03338932\n  0.10397213 -0.04111913 -0.0156139  -0.00452521]": "- [ ] Add copies of the data",
    "[-0.00716995  0.01096196  0.03091057 -0.02336172  0.03515914 -0.07767905\n  0.08191357  0.04285938 -0.04699966 -0.03111143]": "- [ ] Remember to pass the MAX_ITER value to the model, in order to control the while loop for neg sampling ==IMPORTANT==\n- [ ] There is a for loop inside the neg sampling function that is bothering me",
    "[-0.00353277  0.05027919  0.11353061 -0.01824215  0.11050551 -0.05287175\n  0.08559012  0.00277484  0.032343    0.03094645]": "# 24/05/24\n### ==Todo==\n- [ ] ONLY THE CORRUPTION OF TAIL ENTITIES IS IMPLEMENTEDDDDD\n\n# 03/06/24\n### ==Todo==\n- [x] Include option to remove Top",
    "[-0.03343888  0.01583978  0.07104943 -0.02680744  0.01938805 -0.06618939\n  0.09246109  0.02489284 -0.00026228 -0.00610167]": "- [x] Fix the creation of mu function with top (remove top as a dimension)",
    "[-0.05638147  0.08368293  0.03851203 -0.07497722  0.01358911 -0.11617189\n  0.07287977 -0.00811196 -0.03457968 -0.00129718]": "- [ ] Fix negative sampling to include corruption of head",
    "[ 0.00284182  0.01450614  0.03729714 -0.01118892  0.04562202 -0.07333043\n  0.08558717  0.02268709 -0.05749713 -0.03365716]": "- [ ] Remember to pass the MAX_ITER value to the model, in order to control the while loop for neg sampling ==IMPORTANT== (IT IS CURRENTLY HARD CODED)",
    "[-0.01389046 -0.00463299  0.0434387   0.05034257  0.03692396  0.05543265\n  0.08998047  0.05261237  0.00338785 -0.05182689]": "# 23/04/24",
    "[-0.01292307 -0.04692983  0.11914477  0.00691708  0.05524005 -0.00145795\n  0.09586929  0.04199052  0.07249991  0.02927331]": "Two main issues that need to be observed when relating Wittgenstein to machine learning, and NLP is particular: a) training a model for autoregressive generation via minimizing then negative log likelihood does not capture the notion of meaning as **use**",
    "[-0.08283133 -0.02062172  0.06243046 -0.04744392  0.03883005 -0.00478892\n  0.0355577   0.05403357 -0.00659436  0.02892388]": "b) the opposition between interpolating between seen examples and extrapolating via a fixed, algorithmic procedure (symbolic reasoning) is not the same as found in Wittgenstein",
    "[-0.01499269 -0.0302057  -0.02159614 -0.00764537 -0.0800341  -0.00203726\n -0.08769262 -0.02631858  0.04441698 -0.02344463]": "The question of how to deal with (potentially) infinite inputs is answered by the symbolist in a straightforward manner: you write a (procedural) program, and it is able to generalize to (potentially) infinitely many examples, as it is the correct abstract",
    "[-0.0677532  -0.01952293  0.00949605  0.04378426 -0.00461081 -0.03732448\n -0.01277923  0.01410167  0.08381564 -0.01938788]": "On the other hand, coming up with such abstractions is hard, and, what is more, they are brittle to real life situations",
    "[ 0.00856885  0.00254644  0.00794154  0.03503358 -0.02109624 -0.0731991\n  0.01297693  0.07166702 -0.02882255  0.04587516]": "The main problem with this account is that, many of the hard reasoning problems that come naturally to mathematicians, logicians, and computer scientists, are simply impossible to the layman, even the expert makes mistakes",
    "[ 0.11183754 -0.0340035   0.01805414  0.09545611 -0.01061522  0.01008473\n  0.04023949  0.01695418 -0.03945906 -0.02481693]": "This can be countered by the symbolist through an appeal to Chomsky's competence/performance distinction: an individual may possess the correct abstraction (competence), yet applying it to the concrete case is subject to physical limitations: tiredness, no",
    "[-0.01551857 -0.0126912  -0.00120898  0.00845403 -0.00598004  0.01897216\n  0.0831759   0.12110717  0.11419912  0.02397688]": "I think Wittgenstein tries to tackle the subject in a way that goes over such considerations, and his way of dealing with the problem of potential infinity is closely tied to the notion of meaning as use",
    "[-0.13719214 -0.03132047 -0.03499373  0.04161688  0.04282706  0.06122595\n -0.0651675   0.00622476  0.08065364  0.04227134]": "As such, any solution that aims to create a model for causal generation by taking a use-based point of view must also be able to deal with the problem of potential infinity",
    "[ 0.00808397  0.04874543  0.03923876 -0.04041213 -0.02048958 -0.05530367\n  0.19098371 -0.03393604 -0.02411639 -0.00781684]": "- [x] Check for typos",
    "[-0.05826486  0.03926234 -0.03024346 -0.04292839  0.08966463 -0.03297695\n  0.00744244  0.02742506  0.02520686  0.04939637]": "- [x] Include DOIs\n- [x] Create camera-ready\n- [ ] Create poster",
    "[-0.1142341   0.05668204  0.01264789 -0.0525959   0.00035732 -0.04880289\n  0.18677507 -0.08301543 -0.07035723 -0.0957144 ]": "- [ ] Get job",
    "[ 0.01077643  0.07180174  0.03094087 -0.00651777  0.06405986  0.00125926\n  0.07092633  0.00150365 -0.03461701  0.00947721]": "- [ ] Pass stupid social exam in Norway",
    "[-0.06714523 -0.0073571   0.06095476 -0.03482327  0.07288062 -0.08111415\n  0.07429711  0.02170852 -0.02833222 -0.05628251]": "- [ ] Ask for confirmation of Level 2 in Norwegian",
    "[ 0.02997976  0.02303399  0.0472634  -0.06279451  0.00686073  0.01166277\n  0.07932592 -0.0460236  -0.07622539 -0.04022844]": "- [ ] Apply for permanent residence",
    "[ 0.02481258  0.01688893  0.00652993 -0.0008876   0.0552445   0.03700785\n  0.0158151  -0.01991127 -0.10040487  0.001128  ]": "- [ ] I need to report to the police around 20 october",
    "[-0.05264809  0.00355178  0.01165899 -0.0424937   0.0295124  -0.0059434\n -0.00302428 -0.00735819 -0.08482025 -0.06476335]": "- [ ] I need this card around the end of september",
    "[-0.0159357   0.01409907  0.06204316  0.00569546  0.05049963 -0.03937375\n  0.1100508   0.03315144 -0.0976226  -0.06748923]": "### Meta ML engineer interview tips\n\n1",
    "[-0.02286002  0.02129938  0.07406463 -0.03885953  0.10737384 -0.00248348\n  0.06809822  0.01329214  0.0686895  -0.00561227]": "\ud835\udc0f\ud835\udc2b\ud835\udc1e\ud835\udc29\ud835\udc1a\ud835\udc2b\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27 \ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc1e\ud835\udc2b\ud835\udc22\ud835\udc1a\ud835\udc25\ud835\udc2c  \n\ud83d\udca1 [https://lnkd",
    "[-0.12374809 -0.02410722  0.01903331 -0.05730291  0.09860983  0.00970652\n -0.01931656  0.01921469 -0.07422468 -0.04318409]": "in/gsrxg95H](https://lnkd",
    "[-0.12491658 -0.02488464  0.0398955  -0.06954877  0.03217291 -0.02381573\n  0.01805356 -0.01022691 -0.01810344 -0.05383104]": "in/gsrxg95H) - video from a Staff Engineer from Sharechat",
    "[-0.03066859  0.08745067  0.0520158   0.0100164   0.1569138  -0.02925183\n  0.06206068  0.02289609 -0.00681094 -0.04908202]": "Covers 80% of what I always tell in interviews  \n\ud83d\udca1 [https://lnkd",
    "[-0.16986963  0.00625805 -0.05485269 -0.00274693  0.08541007 -0.00436869\n  0.00448728  0.05298617 -0.05131073 -0.01218356]": "in/gZkv8sDn](https://lnkd",
    "[-0.11561722  0.08679516  0.01402067  0.0477753   0.10855865  0.00282913\n -0.02720112  0.0324138  -0.0756497  -0.02290164]": "in/gZkv8sDn) - a good resource to form a complete picture of the interview  \n\ud83d\udca1 [https://lnkd",
    "[-1.16956130e-01 -2.50176582e-02 -4.75101173e-02 -3.78175266e-02\n  1.00355856e-01 -2.35535726e-02 -2.73950827e-05  1.40973665e-02\n -2.94908360e-02 -3.45554650e-02]": "in/gY8gbjk3](https://lnkd",
    "[-0.03875061 -0.0804871  -0.01429761 -0.0066464   0.03812123 -0.06238211\n  0.02339036  0.05621402 -0.07927725 -0.00754988]": "in/gY8gbjk3) - if you want to read big tech companies blogs, you can find some links here  \n  \n2",
    "[-0.06986224 -0.07306708  0.03643095  0.00286444  0.01238085 -0.06021946\n  0.0236847   0.00945823  0.01594524 -0.0014235 ]": "\ud835\udc0a\ud835\udc27\ud835\udc28\ud835\udc30 \ud835\udc0c\ud835\udc0b \ud835\udc1b\ud835\udc1a\ud835\udc2c\ud835\udc22\ud835\udc1c\ud835\udc2c  \nIn addition to high-level knowledge of ML system design, there is periodically a chance to demonstrate strong knowledge in ML theory",
    "[-0.00407257  0.03158321 -0.02964002 -0.01029906  0.02736974 -0.07888588\n  0.0345823   0.02043883 -0.03243411 -0.00938953]": "I believe there's no room for error here",
    "[ 0.03942487 -0.10533592 -0.04997401  0.02010335 -0.00960943 -0.02567375\n  0.02006062  0.01616727  0.01773175 -0.06282949]": "If you're discussing a ranking task and can't provide an example of any ML metric, that's an immediate red flag for the interviewer",
    "[-0.07588224 -0.0529591  -0.03136973  0.01162744  0.04886572  0.03644244\n -0.00745161 -0.03904922 -0.00516316  0.03068754]": "Metrics, A/B tests, Random Forest vs Gradient Boosting, regularization, and so on",
    "[ 0.06097702 -0.05461815  0.04451888  0.05926423  0.01335276 -0.02785858\n -0.07405315  0.04629892 -0.121851    0.02108998]": "These are basic things, spend a few months to study all of this if you're still struggling",
    "[-0.07036614 -0.00718169  0.09221141  0.06158955  0.043828   -0.09805387\n  0.06156914 -0.06029442  0.05643034 -0.00514148]": "\ud835\udc04\ud835\udc31\ud835\udc29\ud835\udc1e\ud835\udc2b\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc1e  \nDuring interviews, solving an end-to-end ML task is expected",
    "[-0.04580176  0.05438435  0.01719954 -0.01351771  0.07874065  0.0220872\n -0.00241622  0.01351701  0.00656024  0.03213477]": "Therefore, it's very beneficial if you also solve end-to-end tasks at work",
    "[ 0.0060702  -0.0095266   0.0032701   0.04713053  0.05073809 -0.11507153\n -0.15341091 -0.01379895  0.06487177  0.00150113]": "Unfortunately, this is often not the case, and most of the time is spent on one aspect, whether it's models quality research, data pipelines, or model optimization in production",
    "[ 0.03385664 -0.04005387  0.03280348  0.03162641 -0.03127511 -0.02068882\n -0.01445605 -0.01471159 -0.01783777 -0.03152286]": "I recommend being involved in all aspects to gain a more comprehensive experience",
    "[ 0.04866481  0.04110685  0.01308304 -0.05296571  0.02628349  0.01408869\n  0.0107921   0.01484089  0.10949522 -0.00272219]": "It also helps a lot to communicate with adjacent teams, understanding how they solve their problems",
    "[ 0.00822926 -0.0720102  -0.04221512 -0.00599259  0.03341235 -0.03799612\n -0.01080886 -0.048306    0.08589854 -0.07169833]": "For example, you can organize regular ML talks where different teams share their experiences",
    "[ 0.04689901  0.00025664 -0.08567221  0.0070252  -0.06945486  0.00230716\n  0.06537843  0.00988499  0.00209021 -0.04645902]": "4",
    "[ 0.0037759  -0.00787709  0.05946865  0.0733375   0.07099454 -0.09879987\n  0.0230157  -0.00878845 -0.01279516 -0.01210959]": "\ud835\udc08\ud835\udc27\ud835\udc2d\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc29\ud835\udc1e\ud835\udc28\ud835\udc29\ud835\udc25\ud835\udc1e  \nIt so happened that before applying, I was actively interviewing people and conducted over 50 ML interviews",
    "[ 0.07761348 -0.01351345  0.01184218  0.05122234  0.01803489  0.02267796\n  0.02679751  0.06753839 -0.03783584 -0.03596743]": "This helps a lot",
    "[ 0.09614416 -0.0181984   0.0104804   0.01246964  0.04510775 -0.02098474\n  0.03497607  0.00137118  0.02016829 -0.00940237]": "Firstly, my conscience doesn't allow me to ask questions that I haven't thoroughly studied",
    "[-0.00899529  0.00248533  0.01259737  0.0047766   0.12925868  0.04767809\n -0.06192006 -0.02715683  0.03749654 -0.04920378]": "Secondly, candidates constantly give different answers and explanations, which helps to delve deeper into the topic",
    "[-0.04266143 -0.03950455  0.01421785  0.06967098  0.06261253 -0.07639771\n  0.0014836  -0.07468413 -0.02103641 -0.0458561 ]": "Thirdly, people with experience from various fields come for interviews, and you can always learn something new",
    "[-0.01240885 -0.02487964  0.06662568  0.01144781  0.06731303 -0.07329001\n  0.08013683 -0.04424141 -0.02292584 -0.0445721 ]": "\ud835\udc0a\ud835\udc1a\ud835\udc20\ud835\udc20\ud835\udc25\ud835\udc1e  \nNot everyone has the opportunity to gain relevant experience at work",
    "[-0.10267472 -0.03955304 -0.02814061 -0.00980805 -0.0578718   0.0295225\n  0.05883284  0.0816617  -0.05085821  0.02010157]": "in that case, I recommend Kaggle",
    "[-0.12689395 -0.01211535  0.02870307 -0.01139206 -0.0024454   0.08045638\n  0.08242211  0.06131888 -0.11165745 -0.02000988]": "Although I don't have any notable achievements on Kaggle, I found it very useful when I wanted to quickly get hands-on experience in recommendations",
    "[-0.0173862  -0.02300929  0.02676233  0.03682365  0.00455378 -0.01815765\n  0.02249847 -0.03675391 -0.06239423 -0.08591269]": "You can also learn interesting ML tricks there, which are sometimes appropriate to mention in interviews",
    "[-0.00332167  0.00444393 -0.08935944  0.04227474 -0.04988597  0.06009243\n  0.02196705  0.0497658  -0.00714873 -0.02621223]": "6",
    "[-0.04616028  0.01541667  0.07620326  0.01655362  0.09335568 -0.08753617\n  0.06199739 -0.01092662  0.07838394 -0.09988349]": "\ud835\udc08\ud835\udc27\ud835\udc2d\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30 \ud835\udc29\ud835\udc25\ud835\udc1a\ud835\udc27  \nDuring the interview, it's expected that you'll be speaking 90% of the time and will completely lead the conversation",
    "[ 0.02751121  0.06039804  0.04784001  0.00245294  0.09286933  0.02600772\n  0.02364247 -0.02931483  0.01490347  0.01910486]": "Therefore, it's essential to have a clear plan to follow",
    "[-0.10268367  0.05109058  0.00557882  0.07362367  0.08991947 -0.02760034\n  0.009363    0.03740275 -0.0313474  -0.04457077]": "You can see my interview plan in the comments",
    "[ 0.0131378   0.1049098   0.01746467 -0.04595516 -0.07587236  0.05052282\n  0.14026803  0.02203989 -0.05640075 -0.05767821]": "Business problem understanding and clarifying questions  \n2",
    "[-0.01623699 -0.10107816  0.01005973 -0.02821678  0.08106661 -0.00345177\n  0.02412061 -0.00229881 -0.02954915 -0.01659508]": "ML task formulation and offline metrics  \n3",
    "[ 0.00011648  0.04338367 -0.00140399 -0.0764439  -0.01811514 -0.02307893\n  0.00682688  0.0036717  -0.08100252 -0.030535  ]": "[Optional] High-level architecture  \n4",
    "[-0.0244784   0.04958897 -0.05221317 -0.0296669   0.01130729  0.00389135\n  0.02989222 -0.00944251 -0.04533156  0.03486511]": "Data collection, filtering, analysis, storing  \n5",
    "[-0.01433526 -0.00808497  0.05994216 -0.00098862  0.018318    0.05353645\n -0.01083266 -0.00946232 -0.16420248 -0.00568742]": "Features preparation  \n6",
    "[-0.02700851 -0.11602847  0.04594621  0.02534072 -0.03033342 -0.02575187\n -0.01585365 -0.01994518 -0.04715406 -0.04428378]": "ML modeling  \n7",
    "[-0.02860087 -0.07527248 -0.05598175  0.0090626  -0.07192365  0.01746628\n -0.01524179  0.0601934   0.02379927 -0.00821543]": "Online metrics, A/B testing  \n8",
    "[ 0.03949602 -0.05810397  0.03230391  0.00617589  0.0506303  -0.10410216\n -0.01334646  0.01013785 -0.0490241   0.04955338]": "Deployment, online learning, MLOps, model optimization, monitoring, logging, etc",
    "[-0.01630761 -0.07287694 -0.0205808  -0.019436   -0.03389727 -0.00912514\n  0.03718855  0.07027282  0.11258601 -0.03792578]": "As you can see, I like to discuss metrics almost at the very beginning",
    "[-0.01863114  0.02769465  0.0094485   0.0138611   0.00583553  0.09549284\n  0.00566009 -0.07370406  0.06870855  0.03289932]": "This is optional, the order can be changed",
    "[-0.04172041  0.08672561  0.04622209 -0.01293598  0.01435079 -0.03546058\n -0.09537392 -0.02295736 -0.0883424  -0.04040068]": "Also, do not forget that this is an approximate plan, and you need to act depending on the interview, feel which steps should be discussed longer",
    "[ 0.02563187  0.0303249   0.01442113 -0.06035993 -0.02416554 -0.03475789\n -0.07744384  0.03985779 -0.015919   -0.01788088]": "And the plan can be iterative, you can quickly go through all the steps and then discuss some of them in more depth",
    "[-0.01470265  0.02119682 -0.0094668  -0.06904821  0.00425626  0.01353532\n -0.12296093  0.05724884 -0.03570921 -0.08402994]": "High-level architecture is necessary when you are asked to design search and it is reasonable to discuss several steps like candidate generation, ranking, reranking and so on",
    "[ 0.00773856 -0.00119177 -0.06130052 -0.0751849   0.01895295  0.1511818\n  0.01027735  0.08463214  0.08463446 -0.02433241]": "Main idea: train a model on the text representation of a chess match and predict the mean Elo of the players",
    "[-0.01432995  0.09342959 -0.04358571 -0.00171432 -0.03124154  0.0009502\n  0.0030973   0.0092904  -0.04556557  0.04013592]": "Steps: a) gather the data",
    "[-0.04333959  0.08601993 -0.05650321 -0.02366242 -0.04725876 -0.00484877\n  0.03344623 -0.00045287  0.00160345  0.08470393]": "b) pre-process the data",
    "[ 0.03886646  0.06743145 -0.03861599 -0.09554773 -0.02723167  0.05756384\n  0.14547902  0.0437833   0.02014518  0.08409472]": "c) tokenize the games",
    "[-0.00066885  0.00345082 -0.04554411  0.02404441 -0.07231899 -0.02435933\n  0.12632346  0.01853536 -0.03822139  0.01110526]": "d) create a Dataset type object",
    "[-0.04063656  0.04146122 -0.02521506  0.02657067 -0.00147436  0.07149053\n  0.01024527  0.04086182  0.06059108  0.02485132]": "e) define the model",
    "[-0.02167685  0.06148202 -0.06442957  0.01108387 -0.05871125  0.06154038\n  0.0615188   0.04218328 -0.06259664 -0.02482598]": "f) create the training routine",
    "[-0.04556906  0.11802641 -0.05937166 -0.02227734 -0.05700276  0.01392793\n  0.0685584   0.03872103 -0.02385169  0.02498538]": "g) define an evaluation procedure",
    "[-0.06246799  0.02025836 -0.0055396  -0.02680916  0.06110628  0.06732751\n  0.04826093 -0.01228562 -0.00652135 -0.05225626]": "h) train the models and pick the best one based on validation eval",
    "[-0.02077191 -0.04956896 -0.12049055 -0.0818067  -0.01168166 -0.0315518\n -0.0914525   0.03825554 -0.05314344  0.01962629]": "i) deploy the model using Gradio or a similar service",
    "[ 0.00052556 -0.02384095 -0.03584672  0.03928227 -0.03083214 -0.01291545\n -0.00053831  0.09008268 -0.12127123  0.00350796]": "### Gather the data\n\n### Pre-process the data\n\n### Tokenizing the data\n\n### Creating a dataset\n\n### Defining the model\n\n### Creating the training routine\n\n### Defining an evaluation procedure\n\n### Train the models and pick the best one\n\n### Deploying the m",
    "[-0.12007936  0.06370979  0.0322514   0.0416418   0.03050787  0.01655654\n -0.02078328 -0.0338469   0.03406157 -0.01394893]": "Sentence transformers used page: https://huggingface",
    "[-0.0044578   0.06257164 -0.00943195 -0.0371368   0.02598124  0.09322089\n -0.0566283   0.04643263  0.01535476 -0.01196065]": "co/sentence-transformers/all-MiniLM-L6-v2",
    "[-0.04047877 -0.01042674  0.03491386 -0.02879747 -0.05477488  0.03594607\n -0.04571729  0.12459684  0.04603092 -0.00089169]": "Sentence transformers max token: The sequence length was limited to 128 tokens",
    "[-0.00472032  0.03331528 -0.07068958  0.02453823 -0.01138055 -0.02327839\n -0.07065216  0.02449739  0.04180606 -0.02613484]": "https://www",
    "[-0.03901393 -0.05878728 -0.01332376  0.01397111  0.03364282  0.03867835\n  0.10103267  0.08748379 -0.02022308 -0.05906974]": "sbert",
    "[-0.03076765 -0.07674132 -0.06783476  0.04058109  0.01375432  0.02283025\n  0.04844712  0.09845833 -0.01643746  0.01054922]": "net/examples/applications/retrieve_rerank/README",
    "[-0.07773907  0.01999448 -0.02530189  0.04386556  0.02363948  0.02420372\n -0.05171052 -0.0434273   0.03069928 -0.02529657]": "html look into a Retrieve & Rerank strategy",
    "[-0.01880125 -0.00634814 -0.0037057   0.04695106 -0.01769501 -0.03266481\n -0.07244056  0.03727152  0.05035249 -0.006178  ]": "Symmetric vs Asymmetric search: symmetric means the query and the answer have roughly the same length, while in asymmetric search you typically have a short query and the answer is much longer",
    "[-0.03665924  0.07688093 -0.01756649 -0.00188788 -0.00509056  0.07028962\n  0.07547561  0.07685261  0.0579054   0.00135922]": "# 23/05/24\n- [ ] Reconhecimento maior e menor",
    "[-0.02368584  0.04074245 -0.05318738 -0.04722494 -0.05782879  0.02566253\n  0.05951265  0.0005662  -0.03089558 -0.02452055]": "- [ ] Reconhecer progress\u00e3o",
    "[ 0.05200512 -0.02477335 -0.0704639  -0.03482866 -0.05516891  0.0127883\n  0.00782469  0.0160621  -0.04794039  0.02923602]": "- [ ] Treinar progress\u00f5es maiores e menores, com acidentes"
}